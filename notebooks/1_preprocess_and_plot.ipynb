{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DRAP`\n",
    "\n",
    "### Database of Remote Affective Physiological Signals and Continuous Ratings Collected in Virtual Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook containing the postprocessing stages for the DRAP dataset.\n",
    "\n",
    "It transforms individual files containing events (.json) and physiological responses (.csv) per each participant, and produces a single file `Dataset_DRAP_full_postprocessed.csv` synchronizing: \n",
    "\n",
    "1) Amplitude from Physiological responses. \n",
    "2) Affect ratings.\n",
    "3) Start and End of intervention stages Resting and Video for three types of video content: \n",
    "    - VideoNegative\n",
    "    - VideoPositive\n",
    "    - VideoNeutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Path: e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/\n"
     ]
    }
   ],
   "source": [
    "# Add files to sys.path\n",
    "from pathlib import Path\n",
    "import sys,os\n",
    "this_path = None\n",
    "try:    # WORKS WITH .py\n",
    "    this_path = str(os.path.dirname(os.path.abspath(__file__)))\n",
    "except: # WORKS WITH .ipynb\n",
    "    this_path = str(Path().absolute())+\"/\" \n",
    "print(\"File Path:\", this_path)\n",
    "\n",
    "# Add the level up to the file path so it recognizes the scripts inside `drap`\n",
    "sys.path.append(os.path.join(this_path, \"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classes\n",
    "from drap.utils import files_handler  # Utils for generation of files and paths\n",
    "import drap.preprocessing       # Generate dataset index, load files, and plots.\n",
    "\n",
    "# Import data science libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General configuration\n",
    "\n",
    "# Path to the participants' folder w.r.t this notebook's filepath\n",
    "DATASET_ROOT_FOLDER = \"../data/\"\n",
    "\n",
    "# Used to generate the path of temporary subfolders\n",
    "DATASET_NAME = \"DRAP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to generate filepaths\n",
    "\n",
    "# MAIN FOLDERS FOR OUTPUT FILES\n",
    "ROOT = this_path + \"\"   # Root folder for all the files w.r.t this file\n",
    "TEMP_FOLDER = ROOT+\"temp/\"  # Main folder for temp files with intermediate calculations\n",
    "RESULTS_FOLDER = ROOT+\"results/\"    # Folder to recreate plots and results from analyses\n",
    "\n",
    "# Generates paths for the temporary files created from this script\n",
    "def gen_path_temp(filename, extension, subfolders=\"\"):\n",
    "    # Generates full paths for TEMP FILES just by specifying a name\n",
    "    return files_handler.generate_complete_path(filename, \\\n",
    "                                        main_folder=TEMP_FOLDER, \\\n",
    "                                        subfolders=DATASET_NAME+\"/\"+subfolders, \\\n",
    "                                        file_extension=extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Load and preprocess datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forcing index construction:  e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/drap_tree_index.json\n",
      "\n",
      "Directory >> participant_101\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_101\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_216\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_216\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_219\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_219\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_222\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_222\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_246\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_246\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_247\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Event>> video_5 - Copy\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_247\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_248\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_248\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_268\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_268\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_270\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_270\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_278\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_278\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_290\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_290\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_293\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_293\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_299\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_299\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_307\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_307\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_308\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_308\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_309\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_309\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_310\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_310\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_312\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_312\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_313\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_313\\compiled_emotion_ratings.csv\n",
      "\n",
      "Directory >> participant_314_v2\n",
      "\t Data>> fast_movement\n",
      "\t Event>> fast_movement\n",
      "\t Data>> slow_movement\n",
      "\t Event>> slow_movement\n",
      "\t Data>> video_1\n",
      "\t Event>> video_1\n",
      "\t Data>> video_2\n",
      "\t Event>> video_2\n",
      "\t Data>> video_3\n",
      "\t Event>> video_3\n",
      "\t Data>> video_4\n",
      "\t Event>> video_4\n",
      "\t Data>> video_5\n",
      "\t Event>> video_5\n",
      "\t Events compiled in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/participant_314_v2\\compiled_emotion_ratings.csv\n",
      "A total of 20 folders were found in the dataset\n",
      "JSON file was created in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/drap_tree_index.json\n",
      "Json file with index of the dataset was saved in e:\\dsv\\dev\\git_repos\\DRAP\\notebooks/temp/drap_index/drap_tree_index.json\n",
      "Participant 0 with folder id: 101 was part of protocol: v1\n",
      "Participant 1 with folder id: 216 was part of protocol: v1\n",
      "Participant 2 with folder id: 219 was part of protocol: v1\n",
      "Participant 3 with folder id: 222 was part of protocol: v1\n",
      "Participant 4 with folder id: 246 was part of protocol: v1\n",
      "Participant 5 with folder id: 247 was part of protocol: v1\n",
      "Participant 6 with folder id: 248 was part of protocol: v1\n",
      "Participant 7 with folder id: 268 was part of protocol: v1\n",
      "Participant 8 with folder id: 270 was part of protocol: v1\n",
      "Participant 9 with folder id: 278 was part of protocol: v1\n",
      "Participant 10 with folder id: 290 was part of protocol: v1\n",
      "Participant 11 with folder id: 293 was part of protocol: v1\n",
      "Participant 12 with folder id: 299 was part of protocol: v1\n",
      "Participant 13 with folder id: 307 was part of protocol: v1\n",
      "Participant 14 with folder id: 308 was part of protocol: v1\n",
      "Participant 15 with folder id: 309 was part of protocol: v1\n",
      "Participant 16 with folder id: 310 was part of protocol: v1\n",
      "Participant 17 with folder id: 312 was part of protocol: v1\n",
      "Participant 18 with folder id: 313 was part of protocol: v1\n",
      "Participant 19 with folder id: 314 was part of protocol: v2\n"
     ]
    }
   ],
   "source": [
    "data_loader = drap.preprocessing.Manager(DATASET_ROOT_FOLDER, \n",
    "                                    index_files_path = TEMP_FOLDER, #TEMP_FOLDER, # None,\n",
    "                                    force_index_regeneration=True, \n",
    "                                    verbose = True,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.events[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum = None\n",
    "for pid,pdata in data_loader.index.items():\n",
    "    print(f\"Participant {pid} with folder id: {pdata['participant_id']} was part of protocol: {pdata['protocol']}\")\n",
    "\n",
    "    # Summary grouped per session segment    \n",
    "    for segtype in data_loader.events[pid][\"Session\"].unique():\n",
    "        \n",
    "        ######\n",
    "        # Summary of events\n",
    "        events_data = data_loader.events[pid]\n",
    "        Q = ( (events_data[\"Session\"] == segtype) )\n",
    "        event_filtered = events_data[Q].reset_index()\n",
    "\n",
    "        ######\n",
    "        # Summary of emotions\n",
    "        emotions_data = data_loader.emotions[pid]\n",
    "        Q = ( (emotions_data[\"Session\"] == segtype) )\n",
    "        emotions_filtered = emotions_data[Q].reset_index()\n",
    "\n",
    "        # Summarize dataframe\n",
    "        current_summary = {\n",
    "                    # \"index_id\": pid,\n",
    "                    # \"participant_id\": pdata['participant_id'],\n",
    "                    # \"protocol\": pdata['protocol'],\n",
    "                    \"Segment\":segtype,\n",
    "                    \"Events_N\": event_filtered.shape[0],\n",
    "                    \"Events_duration\": event_filtered[\"Time\"].iloc[-1] - event_filtered[\"Time\"].iloc[0],\n",
    "                    \"Emotions_N\": emotions_filtered.shape[0],\n",
    "                    \"Emotions_duration\": (np.array(emotions_filtered[\"Time\"])[-1] - np.array(emotions_filtered[\"Time\"])[0]) if emotions_filtered.shape[0]>0 else np.nan,\n",
    "                    \"Emotions_Valence_avg\": emotions_filtered[\"Valence\"].mean(),\n",
    "                    \"Emotions_Arousal_avg\": emotions_filtered[\"Arousal\"].mean()\n",
    "                }\n",
    "\n",
    "        # Convert from values to list, to adapt to DataFrame\n",
    "        current_summary = { k:[v] for k,v in current_summary.items() }\n",
    "        current_summary = pd.DataFrame.from_dict(current_summary)\n",
    "\n",
    "        df_sum = current_summary if (df_sum is None) else pd.concat([df_sum, current_summary], ignore_index=True)\n",
    "    \n",
    "    # Insert participant's data\n",
    "    current_summary.insert(0, \"protocol\", value=pdata['protocol'])\n",
    "    current_summary.insert(0, \"participant_id\", value= pdata['participant_id'])\n",
    "    current_summary.insert(0, \"index_id\", value=pid)\n",
    "    \n",
    "# Print\n",
    "df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experimental events data\n",
    "# data_loader.load_event_files()\n",
    "for id,events in data_loader.events.items():\n",
    "    print(f\"Participant {id} with folder id: {data_loader.index[id]['participant_id']} has in total # events: {events.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.emotions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load emotions data\n",
    "data_loader.load_emotion_files()\n",
    "for id,emotions in data_loader.emotions.items():\n",
    "    print(f\"Participant {id} with folder id: {data_loader.index[id]['participant_id']} has in total # self-reported emotional values: {emotions.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.events[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.emotions[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load segments data\n",
    "data_loader.load_segments_files()\n",
    "for id,segments in data_loader.segments.items():\n",
    "    print(f\"Participant {id} with folder id: {data_loader.index[id]['participant_id']} has in total # segments: {segments.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.segments[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensors placement\n",
    "\n",
    "![EmteqMaskSensors](./datasets/EmteqlabReferenceSensors.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to load all variables types from a single facial EMG muscle Center Corrugator\n",
    "COLNAMES_CENTER_CORRUGATOR=DRAP.GetColnamesFromEmgMuscle(DRAP.EmgMuscles.CenterCorrugator)\n",
    "data, metadata = data_loader.load_data_from_participant(participant_idx = 0, session_part=\"video_1\", columns = COLNAMES_CENTER_CORRUGATOR)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column names for all variables of interest in the rest of the analysis\n",
    "VARS_OF_INTEREST = DRAP.GetColnamesBasicsNonEmg() + DRAP.GetColnamesFromEmgVariableType(DRAP.EmgVars.Amplitude)\n",
    "VARS_OF_INTEREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, metadata = data_loader.load_data_from_participant(participant_idx = 0, session_part=\"video_1\", columns = VARS_OF_INTEREST)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory visualization of all data samples\n",
    "data.iloc[2000:].plot.line(subplots=True, figsize=(20,1*data.shape[1]), sharex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over all participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total participants\n",
    "participants_ids = data_loader.index.keys()\n",
    "participants_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total sessions\n",
    "experimental_stages_ids = [ str(session) for session in data_loader.SessionSegment ]\n",
    "experimental_stages_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Test to know how long it would take to load all the dataset of interest\n",
    "# import time\n",
    "# for participant in participants_ids:\n",
    "#     for exp_stage in experimental_stages_ids:\n",
    "#         t0 = time.time()\n",
    "#         data, metadata = data_loader.load_data_from_participant(participant_idx = participant, session_part = exp_stage, columns=VARS_OF_INTEREST)\n",
    "#         print(f\"\\t>> Loading time: {time.time()-t0} s\")\n",
    "\n",
    "# print(\"\\n\\n=======\\nFinished loading all relevant data!\") \n",
    "\n",
    "# ### It took around 4.5mins just loading all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the sequence order of the videos\n",
    "\n",
    "for SYNC_TIME_USER_ID in participants_ids:\n",
    "    MASK_QUERY = ( \n",
    "                # data_loader.events[SYNC_TIME_USER_ID].Event.str.startswith( \"Playing category number: \" ) | \\\n",
    "                    # data_loader.events[SYNC_TIME_USER_ID].Event.str.startswith( \"Video category finished\" ) | \\\n",
    "                        # data_loader.events[SYNC_TIME_USER_ID].Event.str.startswith( \"Playing rest video\" ) | \\\n",
    "                            # data_loader.events[SYNC_TIME_USER_ID].Event.str.startswith( \"Finished playing rest video\" ) \\\n",
    "                    data_loader.events[SYNC_TIME_USER_ID].Event.str.startswith( \"Playing\" ) \\\n",
    "                    )\n",
    "    \n",
    "    # Event sequence\n",
    "    EVENT_TEXT_SEQUENCE = \"Category sequence:\"\n",
    "    keys_containing_sync_event = data_loader.events[SYNC_TIME_USER_ID].Event.str.startswith(EVENT_TEXT_SEQUENCE)\n",
    "    cat_sequence = data_loader.events[SYNC_TIME_USER_ID][ keys_containing_sync_event ].iloc[0] # Choose first event\n",
    "    video_seq = cat_sequence.Event.split(\":\")[1].split(\",\")\n",
    "\n",
    "    print(f\"Participant: {SYNC_TIME_USER_ID}, Events: {data_loader.events[SYNC_TIME_USER_ID][MASK_QUERY].shape}, Seq: {video_seq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing pipeline to merge physiological and continuous affective ratings\n",
    "\n",
    "The merging pipeline assumes a Participant ID, and the experimental stage to process (*VideoNegative, VideoNeutral, VideoPositive*)\n",
    "1. Identify the timestamps for the resting stage $[r_{t0},r_{t1}]$ and the stage watching the video $[v_{t0},v_{t1}]$\n",
    "2. Find the VideoID of the content being watched at each moment (facilitates filtering per video, if desired)\n",
    "3. Merge the physiological and emotional data with corresponding timestamps.\n",
    "4. Resample the dataframes at 50Hz\n",
    "5. Save the merged dataset in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Participant and Video stage to process\n",
    "PARTICIPANT_IDX = 7\n",
    "EXPERIMENTAL_STAGE_NAME = \"VideoPositive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify timestamps dividing resting and video stages within segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_info_from_segment(df_segments, segment_name_to_filter):\n",
    "    \"\"\"\n",
    "    Processes a dataframe of segments timestamps and returns a tuple with:\n",
    "        - rest_tstamp_start\n",
    "        - rest_tstamp_end\n",
    "        - video_tstamp_start\n",
    "        - video_tstamp_end\n",
    "        - video_filename\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter the segment corresponding to the intended video\n",
    "    df_segments = df_segments[ df_segments.Segment == segment_name_to_filter]\n",
    "\n",
    "    # Find the beginning and end of the RESTING (VideoId == -1)\n",
    "    rest_start = df_segments[ (df_segments.Trigger==\"Start\") & (df_segments.VideoId == -1)].index.min()\n",
    "    rest_end = df_segments[ (df_segments.Trigger==\"End\") & (df_segments.VideoId == -1)].index.max()\n",
    "\n",
    "    # The segment watching the VIDEO (VideoId != -1)\n",
    "    video_start = df_segments[ (df_segments.Trigger==\"Start\") & (df_segments.VideoId != -1)].index.min()\n",
    "    video_end = df_segments[ (df_segments.Trigger==\"End\") & (df_segments.VideoId != -1)].index.max()\n",
    "\n",
    "    # Which file should be loaded to access the required data\n",
    "    video_filename = df_segments.Session.iloc[0]\n",
    "\n",
    "    # Correct the few situations when the video starts before resting ends for few miliseconds\n",
    "    if rest_end > video_start:\n",
    "        video_start = rest_end\n",
    "    \n",
    "    return (rest_start, rest_end, video_start, video_end, video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_t0, r_t1, v_t0, v_t1, video_filename = calculate_info_from_segment(data_loader.segments[PARTICIPANT_IDX], EXPERIMENTAL_STAGE_NAME)\n",
    "print(f\"Participant: \\t\\t{PARTICIPANT_IDX} \\nRest duration: \\t\\t{r_t1-r_t0}s, \\nVideos duration: \\t{v_t1-v_t0} \\nVideoName: \\t\\t{video_filename} \\nResting was first: \\t{r_t0 < v_t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video corresponding to desired Experimental stage\n",
    "data, metadata = data_loader.load_data_from_participant(participant_idx = PARTICIPANT_IDX, session_part = video_filename, columns = VARS_OF_INTEREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter experimental stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter experimental stages\n",
    "data_rest = data[ (data.index >= r_t0) & (data.index < r_t1) ]\n",
    "data_video = data[ (data.index >= v_t0) & (data.index < v_t1) ]\n",
    "\n",
    "print(data_rest.shape, data_video.shape)\n",
    "print(\"Duration stage REST: \", r_t1-r_t0)\n",
    "print(\"Duration stage VIDEO: \", v_t1-v_t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_video.plot.line(subplots=True, figsize=(15,1*data.shape[1]), sharex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load emotional responses within video range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions reported during the video, and in the corresponding video\n",
    "Q = (data_loader.emotions[PARTICIPANT_IDX].index >= v_t0) & \\\n",
    "        (data_loader.emotions[PARTICIPANT_IDX].index < v_t1 ) & \\\n",
    "        (data_loader.emotions[PARTICIPANT_IDX].Session == video_filename)\n",
    "        \n",
    "data_emotions = data_loader.emotions[PARTICIPANT_IDX][ Q ].drop(\"Session\", axis=1)\n",
    "data_emotions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the VideoId per timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_video_id_end_timestamps(df_segments, segment_name_to_filter):\n",
    "    \"\"\"\n",
    "    Returns a dataframe with the timestamp where a given VideoID *finishes*.\n",
    "        - df_segments = DataFrame with segments\n",
    "    \"\"\"\n",
    "    # EVENT_TEXT_SEQUENCE = \"Finished playing video number:\" # It will return when the event finished!\n",
    "    # keys_containing_sync_event = df_events.Event.str.startswith(EVENT_TEXT_SEQUENCE)\n",
    "    # videos_seq = df_events[ keys_containing_sync_event ] # Choose all video numbers\n",
    "    # videos_ending = videos_seq.Event.str.split(\":\")\n",
    "    # video_id_end_timestamp = videos_ending.apply((lambda x: int(x[1])))\n",
    "    # video_id_end_timestamp = pd.DataFrame({\"VideoID\":video_id_end_timestamp})\n",
    "\n",
    "    # Filter the segment corresponding to the intended video\n",
    "    df_segments = df_segments[ df_segments.Segment == segment_name_to_filter]\n",
    "\n",
    "    # Find the end of each video stage\n",
    "    video_id_end_timestamp = df_segments[ (df_segments.Trigger==\"End\") ]\n",
    "    video_id_end_timestamp = video_id_end_timestamp[ [\"VideoId\"] ]\n",
    "    \n",
    "    return video_id_end_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id_end_timestamp = calculate_video_id_end_timestamps(data_loader.segments[PARTICIPANT_IDX], EXPERIMENTAL_STAGE_NAME)\n",
    "video_id_end_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id_end_timestamp.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r_t0, r_t1)\n",
    "print(v_t0, v_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rest = pd.merge_asof(data_rest, video_id_end_timestamp, left_index=True, right_index=True, direction=\"forward\")\n",
    "data_rest[\"VideoId\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_video = pd.merge_asof(data_video, video_id_end_timestamp, left_index=True, right_index=True, direction=\"forward\")\n",
    "data_video[\"VideoId\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rest.insert(0, \"OriginalParticipantID\", data_loader.index[PARTICIPANT_IDX]['folderid'])\n",
    "data_video.insert(0, \"OriginalParticipantID\", data_loader.index[PARTICIPANT_IDX]['folderid'])\n",
    "data_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging physiology with affective ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge physio with subjective emotions\n",
    "data_merged = pd.merge_asof(data_video, data_emotions, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RANGE VIDEO: \\t\\t\", data_video.index.min(), \"---\", data_video.index.max(), \" \\tLength:\", data_video.index.max()-data_video.index.min())\n",
    "print(\"RANGE EMOTIONS: \\t\", data_emotions.index.min(), \"---\", data_emotions.index.max(), \" \\tLength:\", data_emotions.index.max()-data_emotions.index.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_merged.plot.line(subplots=True, figsize=(15,1*data.shape[1]), sharex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample data at 50Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_resampled_dataframe(df, sampling_frequency_hz = 50):\n",
    "    df.index = pd.to_datetime(df.index, unit=\"s\")\n",
    "    _FS = sampling_frequency_hz\n",
    "    df_resampled = df.resample(str(1/_FS)+'S', origin='start').ffill()\n",
    "    # The valence, arousal, rawX, rawY will contain null values before the first value is captured. Fill with first value.\n",
    "    df_resampled = df_resampled.fillna(method=\"backfill\")\n",
    "    # Put the data back to 0 seconds\n",
    "    df_resampled.index -= df_resampled.index[0]\n",
    "    # Transform from datetime to float\n",
    "    df_resampled.index = df_resampled.index.total_seconds()\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to 50Hz\n",
    "FS = 50\n",
    "data_total = calculate_resampled_dataframe(data_merged, FS)\n",
    "data_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "data_total.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_total.plot.line(subplots=True, figsize=(15,1*data.shape[1]), sharex=True)#[0].figure.savefig(gen_path_temp(f\"fig_test_data\", extension=\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the merging process with fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge two fake dataframes with MultiIndex\n",
    "data2 = data_total.copy(deep=True)\n",
    "data3 = data_total.copy(deep=True)\n",
    "\n",
    "# Adding index with participant and video category\n",
    "data_total = pd.concat({(PARTICIPANT_IDX,EXPERIMENTAL_STAGE_NAME):data_total}, names = [\"Participant\",\"VideoCategory\"])\n",
    "data_total.head()\n",
    "# Toy data to prove how to merge fake \"VideoPositive\" data vertically\n",
    "data2 = pd.concat({(PARTICIPANT_IDX,\"VideoPositive\"):data2}, names = [\"Participant\",\"VideoCategory\"])\n",
    "data_total = pd.concat([data_total, data2])\n",
    "# Toy data to combine fake \"VideoNeutral\" vertically\n",
    "data3 = pd.concat({(PARTICIPANT_IDX,\"VideoNeutral\"):data3}, names = [\"Participant\",\"VideoCategory\"])\n",
    "data_total = pd.concat([data_total, data3])\n",
    "data_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data from a single participant and single experimental session\n",
    "data_total.to_csv(gen_path_temp(\"example_df\",extension=\".csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset merging all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading all segments for all participants and store the resting and video parts in a single large CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Where the compiled dataset will be stored\n",
    "DATASET_POSTPROCESSED_FILENAME = gen_path_temp(\"Dataset_DRAP_full_postprocessed\", extension=\".csv\")\n",
    "\n",
    "# Besides the compiled dataset in .CSV, it generates a folder with individual datasets per \n",
    "# participant, and with plots that show the saved data\n",
    "SAVE_SINGLE_FILES_AND_PLOTS = False\n",
    "\n",
    "# Segments of interest. The timestamps that determine each stage will be found and used to segment the physiological data per participant.\n",
    "VIDEO_SEGMENT_NAMES = [\"VideoNegative\", \"VideoPositive\", \"VideoNeutral\"]    # Key of experimental stages\n",
    "PREFIX_RESTING_STAGE = \"Resting_\"\n",
    "FS = 50 # Sampling frequency\n",
    "\n",
    "# Load or create dataframe with statistics of initial dataset\n",
    "dataset_postprocessed_final = None\n",
    "\n",
    "### INPUTS / OUTPUTS\n",
    "\"\"\"EDIT CUSTOM FILENAMES\"\"\"\n",
    "input_files = [DATASET_POSTPROCESSED_FILENAME]\n",
    "\n",
    "# Try to load files maximum two times\n",
    "RELOAD_TRIES = 2\n",
    "for tries in range(RELOAD_TRIES):\n",
    "    try:\n",
    "        ### LOAD FILE\n",
    "        print(f\"Trying {tries+1}/{RELOAD_TRIES} to load files: {input_files}\")\n",
    "        \n",
    "        ### CUSTOM SECTION TO READ FILES\n",
    "        \"\"\"EDIT CUSTOM READ\"\"\"\n",
    "        dataset_postprocessed_final = pd.read_csv(input_files[0])#, index_col=[0,1,2])\n",
    "        print(f\"File {input_files[0]} was successfully loaded\")\n",
    "\n",
    "    except Exception as e:\n",
    "        ### CREATE FILE\n",
    "        print(f\"File not found. Creating again! {e}\")\n",
    "\n",
    "        ### CUSTOM SECTION TO CREATE FILES \n",
    "        \"\"\"EDIT CUSTOM WRITE\"\"\"\n",
    "\n",
    "        for PARTICIPANT_IDX in participants_ids:\n",
    "            for EXPERIMENTAL_STAGE_NAME in VIDEO_SEGMENT_NAMES:\n",
    "\n",
    "                ## Extract segments for specific video type\n",
    "                r_t0, r_t1, v_t0, v_t1, video_filename = calculate_info_from_segment(data_loader.segments[PARTICIPANT_IDX], EXPERIMENTAL_STAGE_NAME)\n",
    "                print(f\"\\n\\nParticipant: \\t\\t{PARTICIPANT_IDX} \\nRest range: \\t\\t{r_t1-r_t0}s, \\nVideos range: \\t{v_t1-v_t0} \\nVideo filename: \\t\\t{video_filename} \\nResting was first: \\t{r_t0 < v_t0}\")\n",
    "\n",
    "                # Load corresponding data and metadata\n",
    "                data, metadata = data_loader.load_data_from_participant(participant_idx = PARTICIPANT_IDX, session_part = video_filename, columns = VARS_OF_INTEREST)\n",
    "                \n",
    "                # Detect the ending timestamp of each VideoID to be added to the datasets\n",
    "                video_id_end_timestamp = calculate_video_id_end_timestamps(data_loader.segments[PARTICIPANT_IDX], EXPERIMENTAL_STAGE_NAME)\n",
    "                \n",
    "                \"\"\" PROCESSING DATA FROM RESTING STAGES \"\"\"\n",
    "                # Filter experimental stages\n",
    "                data_rest = data[ (data.index >= r_t0) & (data.index < r_t1) ]\n",
    "                # Combine the videoId with the data from the segment\n",
    "                data_rest = pd.merge_asof(data_rest,  video_id_end_timestamp, left_index=True, right_index=True, direction=\"forward\")\n",
    "\n",
    "                # Emotions reported during the video, and in the corresponding video\n",
    "                Q = (data_loader.emotions[PARTICIPANT_IDX].index >= r_t0) & \\\n",
    "                        (data_loader.emotions[PARTICIPANT_IDX].index < r_t1 ) & \\\n",
    "                        (data_loader.emotions[PARTICIPANT_IDX].Session == video_filename)\n",
    "                data_emotions_rest = data_loader.emotions[PARTICIPANT_IDX][ Q ].drop(\"Session\", axis=1)\n",
    "                # Merge data end emotions in a single dataframe per time\n",
    "                data_rest = pd.merge_asof(data_rest, data_emotions_rest, left_index=True, right_index=True)\n",
    "\n",
    "                # Resampling data\n",
    "                data_rest_resampled = calculate_resampled_dataframe(data_rest, FS)\n",
    "\n",
    "                \"\"\" PROCESSING DATA FROM VIDEO STAGES \"\"\"\n",
    "                # Filter experimental stages\n",
    "                data_video = data[ (data.index >= v_t0) & (data.index < v_t1) ]\n",
    "                # Combine the videoId with the data from the segment\n",
    "                data_video = pd.merge_asof(data_video, video_id_end_timestamp, left_index=True, right_index=True, direction=\"forward\")\n",
    "                \n",
    "                # Emotions reported during the video, and in the corresponding video\n",
    "                Q = (data_loader.emotions[PARTICIPANT_IDX].index >= v_t0) & \\\n",
    "                        (data_loader.emotions[PARTICIPANT_IDX].index < v_t1 ) & \\\n",
    "                        (data_loader.emotions[PARTICIPANT_IDX].Session == video_filename)\n",
    "                data_emotions = data_loader.emotions[PARTICIPANT_IDX][ Q ].drop(\"Session\", axis=1)\n",
    "                \n",
    "                # Merge data end emotions in a single dataframe per time\n",
    "                data_video = pd.merge_asof(data_video, data_emotions, left_index=True, right_index=True)\n",
    "                # Resample dataset to constant period\n",
    "                data_video_resampled = calculate_resampled_dataframe(data_video, FS)\n",
    "\n",
    "                \"\"\" COMBINING DATASET IN A SINGLE ONE \"\"\"\n",
    "                print(f\"Actual duration stage VIDEO: {data_video_resampled.index.max()} \\tSHORT?:{data_video_resampled.index.max()<295}\")\n",
    "                print(f\"Actual duration stage REST: {data_rest_resampled.index.max()} \\tSHORT?:{data_rest_resampled.index.max()<115}\")\n",
    "                print(f\"Total missing vals:{data_video_resampled.isnull().sum().sum()}\")\n",
    "\n",
    "                # Add a column with the original participant ID corresponding to the original dataset\n",
    "                folder_id = data_loader.index[PARTICIPANT_IDX]['folderid']\n",
    "                data_rest_resampled.insert(0, \"OriginalParticipantID\", folder_id)\n",
    "                data_video_resampled.insert(0, \"OriginalParticipantID\", folder_id)\n",
    "\n",
    "                ### SAVING FILES\n",
    "                if SAVE_SINGLE_FILES_AND_PLOTS:\n",
    "                    # Save resting data\n",
    "                    data_to_plot = {\n",
    "                                        \"video_data\":data_video_resampled,\n",
    "                                        \"resting_data\":data_rest_resampled\n",
    "                                    }\n",
    "                    for _df_name,_df in data_to_plot.items():\n",
    "                        # Save video data\n",
    "                        save_path_plot = gen_path_temp(f\"per_participant/_plots/{folder_id}/{EXPERIMENTAL_STAGE_NAME}_{_df_name}\", extension=\".png\")\n",
    "                        save_path_csv = gen_path_temp(f\"per_participant/{folder_id}/{EXPERIMENTAL_STAGE_NAME}_{_df_name}\", extension=\".csv\")\n",
    "                        _df.plot.line(subplots=True, figsize=(15,1*_df.shape[1]), sharex=True)[0].figure.savefig(save_path_plot); plt.close()\n",
    "                        _df.to_csv(save_path_csv)\n",
    "\n",
    "                ### Generating multiindex to create a single .csv with all the data\n",
    "                COLNAMES_MULTIINDEX = [\"Participant\",\"Stage\"]\n",
    "                data_video_resampled = pd.concat({(PARTICIPANT_IDX,EXPERIMENTAL_STAGE_NAME):data_video_resampled}, names = COLNAMES_MULTIINDEX)\n",
    "                data_rest_resampled = pd.concat({(PARTICIPANT_IDX, PREFIX_RESTING_STAGE + EXPERIMENTAL_STAGE_NAME):data_rest_resampled}, names = COLNAMES_MULTIINDEX)\n",
    "\n",
    "                # Final concatenation of resting and video stages\n",
    "                data_compiled = pd.concat([data_video_resampled.copy(deep=True), data_rest_resampled.copy(deep=True)])\n",
    "\n",
    "                # Generate final DF\n",
    "                if(dataset_postprocessed_final is None):\n",
    "                    dataset_postprocessed_final = data_compiled.copy(deep=True)\n",
    "                    # dataset_postprocessed_final = pd.concat([dataset_postprocessed_final, data_rest_resampled.copy(deep=True)])\n",
    "                else:\n",
    "                    dataset_postprocessed_final = pd.concat([dataset_postprocessed_final, data_compiled.copy(deep=True)])\n",
    "                    # dataset_postprocessed_final = pd.concat([dataset_postprocessed_final, data_rest_resampled.copy(deep=True)])\n",
    "\n",
    "        # Saving .csv\n",
    "        dataset_postprocessed_final.to_csv( DATASET_POSTPROCESSED_FILENAME )\n",
    "        print(\"\\n\\n End\")\n",
    "\n",
    "\n",
    "        ### ---- CONTROL RETRIES\n",
    "        if tries+1 < RELOAD_TRIES:\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    # Finish iteration\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_postprocessed_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_postprocessed_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">> FINISHED WITHOUT ERRORS!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8ae395091c52fa3e7de74e24d0246beaf9010cdf8cc39c2a7225d55042d0be5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
