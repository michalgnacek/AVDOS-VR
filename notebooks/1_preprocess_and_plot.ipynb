{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AVDOS-VR` - Virtual Reality Affective Video Database with Physiological Signals\n",
    "\n",
    "Notebook containing the postprocessing stages for the AVDOSVR dataset.\n",
    "\n",
    "It transforms individual files containing events (.json) and physiological responses (.csv) per each participant, and produces a single file `Dataset_AVDOSVR_full_postprocessed.csv` synchronizing physiological responses, affect ratings, and grouped per affect segment and experimental stages (rest, video)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add files to sys.path\n",
    "from pathlib import Path\n",
    "import sys,os\n",
    "this_path = None\n",
    "try:    # WORKS WITH .py\n",
    "    this_path = str(os.path.dirname(os.path.abspath(__file__)))\n",
    "except: # WORKS WITH .ipynb\n",
    "    this_path = str(Path().absolute())+\"/\" \n",
    "print(\"File Path:\", this_path)\n",
    "\n",
    "# Add the level up to the file path so it recognizes the scripts inside `avdosvr`\n",
    "sys.path.append(os.path.join(this_path, \"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classes\n",
    "import avdosvr.preprocessing       # Generate dataset index, load files, and plots.\n",
    "\n",
    "# Shortcut for general variable constants\n",
    "import avdosvr.utils.enums as avdosEnums\n",
    "# Utils for generation of files and paths\n",
    "from avdosvr.utils import files_handler\n",
    "from avdosvr.analysis.dataframe_functions import resample_dataframe\n",
    "\n",
    "# Import data science libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "Global variables and functions for file management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General configuration\n",
    "\n",
    "# Path to the participants' folder w.r.t this notebook's filepath\n",
    "DATASET_ROOT_FOLDER = \"../data/\"\n",
    "\n",
    "# Used to generate the path of temporary subfolders\n",
    "NOTEBOOK_NAME = \"1_preprocess\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to generate filepaths\n",
    "\n",
    "# MAIN FOLDERS FOR OUTPUT FILES\n",
    "ROOT = this_path + \"\"   # Root folder for all the files w.r.t this file\n",
    "TEMP_FOLDER = ROOT+\"temp/\"  # Main folder for temp files with intermediate calculations\n",
    "RESULTS_FOLDER = ROOT+\"results/\"    # Folder to recreate plots and results from analyses\n",
    "\n",
    "# Generates paths for files created from this script\n",
    "def gen_path_temp(filename, extension, subfolders=\"\"):\n",
    "    # Generates full paths for TEMP FILES just by specifying a name\n",
    "    return files_handler.generate_complete_path(filename, \\\n",
    "                                        main_folder=TEMP_FOLDER, \\\n",
    "                                        subfolders=NOTEBOOK_NAME+\"/\"+subfolders, \\\n",
    "                                        file_extension=extension)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating a dataset index\n",
    "\n",
    "The class `avdosvr.processing.Manager()` contains the scripts to generate an index of the dataset, which facilitates access to the data per participant, event, experimental segment, or physiological variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The preprocessing manager analyzes the original data folder\n",
    "# to create an index and facilitate preprocessing.\n",
    "data_loader = avdosvr.preprocessing.Manager(DATASET_ROOT_FOLDER, \n",
    "                                    index_files_path = TEMP_FOLDER, # None,\n",
    "                                    force_index_regeneration=True, \n",
    "                                    verbose = True,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary dataframe produces the following columns:\n",
    "- `index_id`: Identifier of a participant in the index, based on the order of the folders in the original dataset.\n",
    "- `participant_id`: Participant's identifier according to original folder name.\n",
    "- `protocol`: *v1* or *v2* depending on which type of remote experiment the participant did (see paper for details).\n",
    "- `Segment`: *video_1* to *video_5* identifying the filename of the experimental segment.\n",
    "- `NonAffectiveEvents_N`: Number of events **not** related to self-reported affective ratings.\n",
    "- `NonAffectiveEvents_duration`: The difference between the first and the last non affective event (in seconds).\n",
    "- `AffectiveRatings_N`: Number of events related to self-reported affective ratings.\n",
    "- `AffectiveRatings_duration`: The difference between the first and the last affective event (in seconds). (No ratings as *NaN* in the dataframe)\n",
    "- `AffectiveRatings_Valence_avg`: Average *Valence* ratings throughout a specific experimental *Segment*.\n",
    "- `AffectiveRatings_Arousal_avg`: Average *Arousal* ratings throughout a specific experimental *Segment*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attribute `summary` presents an overview on the original files.\n",
    "# Note that it does not consider synchronization with the \n",
    "# real events in the experiment\n",
    "data_loader.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some rows contain no data, but they are in the resting video_1 (ie. ratings were not collected during training portion of video 1 segment)\n",
    "data_loader.summary[ data_loader.summary.isna().any(axis=1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attribute `index` contains the filepath for each\n",
    "# of the events and physiological data for the participant\n",
    "# with index 0. The index is according to the order how\n",
    "# the files are found in the main folder `data/`\n",
    "data_loader.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attribute `events` contains all events\n",
    "# not related to affective states\n",
    "data_loader.events[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attribute `emotions` contains the events\n",
    "# related to self-reported affective ratings.\n",
    "# The RawX and RawY correspond to the original\n",
    "# values measured with the joystick controller\n",
    "data_loader.emotions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attribute `segments` is a subset of the data stored in\n",
    "# `events`. This one contains the start and ending point\n",
    "# of each of the experimental segments, and the specific VideoId\n",
    "# shown during that experimental segment.\n",
    "data_loader.segments[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading physiological data\n",
    "\n",
    "Below, we show how to access and visualize specific physiological data from the files.\n",
    "\n",
    "Note that the variable names to indicate the facial EMG are based on the placement provided by the Emteq sensor, as shown below:\n",
    "\n",
    "![EmteqMaskSensors](https://www.frontiersin.org/files/Articles/781218/frvir-03-781218-HTML-r2/image_m/frvir-03-781218-g005.jpg)\n",
    "*Image taken from paper: emteqPROâ€”Fully Integrated Biometric Sensing Array for Non-Invasive Biomedical Research in Virtual Reality [DOI](https://doi.org/10.3389/frvir.2022.781218))*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all variables for a specific participant and a specific session segment\n",
    "PARTICIPANT_IDX = 0\n",
    "SESSION_SEGMENT_NAME = str(avdosEnums.SessionSegment.video1) # Or you can type directly the string from session segment `video_1`\n",
    "\n",
    "# Obtain dataframes with data and metadata\n",
    "data, metadata = data_loader.load_data_from_participant(participant_idx = PARTICIPANT_IDX,\n",
    "                                                        session_segment=SESSION_SEGMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "The command above allows loading the raw data as it is from the file. The only normalization is the `Time` to unix timestamps, so that it maps the timestamps from the event files.\n",
    "\n",
    "The attribute `normalize_data_units` allows loading the data with normalized units, as follows:\n",
    "- The EMG variables with type `Raw/`, `Filtered/`, and `Amplitude/` are normalized with the value in `#Emg/Properties.rawToVoltageDivisor` to produce signal in **volts**.\n",
    "- The EMG variables with type `Contact/` are normalized with the value in `#Emg/Properties.contactToImpedanceDivisor` to produce data in **ohms**.\n",
    "- The `Accelerometer/` variables are normalized with `#Accelerometer/Properties.rawDivisor` to produce data in $m/s^2$.\n",
    "- The `Magnetometer/` variables are normalized with `#Magnetometer/Properties.rawDivisor` to produce data in $\\mu$ Tesla.\n",
    "- The `Gyroscope/` variables are normalized with `#Gyroscope/Properties.rawDivisor` to produce data in $^\\circ/s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain normalized data with parameter `normalize_data_units`\n",
    "data, metadata = data_loader.load_data_from_participant(participant_idx = PARTICIPANT_IDX,\n",
    "                                                        session_segment=SESSION_SEGMENT_NAME,\n",
    "                                                        normalize_data_units=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how the range of the values changed to the correct units\n",
    "# compared  to the non-normalize data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting subsets of physiological variables\n",
    "\n",
    "The parameter `columns` allows loading a certain subset of variables from the physiological signal. Especially because there is redundancy in the EMG variables `Raw/`, `Filtered/`, and `Amplitude`.\n",
    "\n",
    "To make it easier to access subset of variables, we show below how to access a specific data channels from the configuration file:\n",
    "\n",
    "- Physical faceplate: `COLNAMES_FACEPLATE`,\n",
    "- Facial EMG: `COLNAMES_EMG_RAW`, `COLNAMES_EMG_FILTERED`, `COLNAMES_EMG_AMPLITUDE`, \n",
    "- Contact states: `COLNAMES_EMG_CONTACT`, `COLNAMES_EMG_CONTACT_STATES`, `COLNAMES_NON_EMG_BASIC`\n",
    "- Heart-related: `COLNAMES_HR`, `COLNAMES_PPG`\n",
    "- Movement-related: `COLNAMES_ACCELEROMETER`, `COLNAMES_MAGNETOMETER`, `COLNAMES_GYROSCOPE`\n",
    "\n",
    "The variable `COLNAMES_RECOMMENDED` contains a suggested set of columns useful for analysis, but you can always select columns by providing a list with the custom names that you would like to load.\n",
    "\n",
    "The function `avdosvr.preprocessing.GetColnamesFromEmgMuscle()` allows to access all variables for a specific muscle type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example: Creating a list of columns to extract all variables from center corrugator,\n",
    "# the amplitude of all sensors, heart rate, and pressur.\n",
    "COLS_OF_INTEREST = avdosvr.preprocessing.GetColnamesFromEmgMuscle(avdosEnums.EmgMuscles.CenterCorrugator) +\\\n",
    "                avdosvr.preprocessing.COLNAMES_EMG_AMPLITUDE +\\\n",
    "                avdosvr.preprocessing.COLNAMES_HR\n",
    "print(COLS_OF_INTEREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain normalized data with parameter `normalize_data_units`\n",
    "data, metadata = data_loader.load_data_from_participant(participant_idx = PARTICIPANT_IDX,\n",
    "                                                        session_segment=SESSION_SEGMENT_NAME,\n",
    "                                                        normalize_data_units=True,\n",
    "                                                        columns = COLS_OF_INTEREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "The resulting `data` is a pandas dataframe and can be plotted as such. See their [official documentation](https://pandas.pydata.org/pandas-docs/stable/reference/plotting.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize each channel separately\n",
    "data.plot.line(figsize=(15,1*data.shape[1]), subplots=True, sharex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over all participants\n",
    "\n",
    "It is commonly interesting to compile data from all participants. However, the resulting dataframe may be large and not fit in memory.\n",
    "We suggest design a preprocessing stage for each participant that generates a smaller dataset, and then join them together for complete analysis.\n",
    "\n",
    "In the example below, we show two examples on how to iterate over the participants' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total participants\n",
    "participants_ids = data_loader.summary[\"index_id\"].unique()\n",
    "participants_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total sessions\n",
    "experiment_segment_names = data_loader.summary[\"Segment\"].unique()\n",
    "experiment_segment_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all participants and segments\n",
    "for participant in participants_ids:\n",
    "    for exp_segment in experiment_segment_names:\n",
    "        print(f\"\\t>> Participant {participant} and segment {exp_segment}\")\n",
    "print(\"\\n\\n=======\\nFinished iterating all relevant data!\") \n",
    "### It takes around 15 mins just loading all the raw dataset one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Uncomment the block below if you want to iterate over the whole dataset. However, the block takes around 15 mins finalizing.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Testing how long it would take to iterate over the whole dataset\n",
    "# ### without including normalization\n",
    "# import time\n",
    "# for participant in participants_ids:\n",
    "#     for exp_segment in experiment_segment_names:\n",
    "#         t0 = time.time()\n",
    "#         data, metadata = data_loader.load_data_from_participant(participant_idx = participant, session_segment = exp_segment)\n",
    "#         print(f\"\\t>> Loading time: {time.time()-t0} s\")\n",
    "# print(\"\\n\\n=======\\nFinished loading all relevant data!\") \n",
    "# ### It takes around 15 mins just loading all the raw dataset one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading specific experimental stage\n",
    "\n",
    "The  experimental stage (*Negative, Neutral, Positive*) was randomized. For some users the file `video_1` corresponds to the videos with `Neutral` affective induction, whereas the same file for another participant may represent the `Positive` stage. \n",
    "\n",
    "We provide scripts to easily access the physiological data per experimental segment, without worrying which specific file to load with the function `calculate_info_from_segment()`.\n",
    "\n",
    "In this example, we will load the participant `PARTICIPANT_IDX`, to load the affective segment `Positive` and process the columns recommended for a comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTICIPANT_IDX = 0\n",
    "AFFECTIVE_SEGMENT = str(avdosEnums.AffectSegments.VideosPositive) # or \"Positive\"\n",
    "COLNAMES_PHYSIO = avdosvr.preprocessing.COLNAMES_RECOMMENDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_t0, r_t1, v_t0, v_t1, video_filename = data_loader.calculate_info_from_segment(PARTICIPANT_IDX, AFFECTIVE_SEGMENT)\n",
    "\n",
    "print(f\"\\n\\\n",
    "Rest duration: \\t\\t{r_t1-r_t0}s \\n\\\n",
    "Videos duration: \\t{v_t1-v_t0} \\n\\\n",
    "Video Name: \\t\\t{video_filename} \\n\\\n",
    "Resting was first?: \\t{r_t0 < v_t0}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `video_filename` extracted from the desider experimental segment above\n",
    "data, metadata = data_loader.load_data_from_participant(participant_idx = PARTICIPANT_IDX, \n",
    "                                                        session_segment = video_filename,\n",
    "                                                        normalize_data_units = True,\n",
    "                                                        columns = COLNAMES_PHYSIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data between stages\n",
    "data_rest = data[ (data.index >= r_t0) & (data.index < r_t1) ]\n",
    "data_video = data[ (data.index >= v_t0) & (data.index < v_t1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop is to verify that the data loaded from each participant has\n",
    "#  the desired length.\n",
    "for pid in participants_ids:\n",
    "    video_sequence = data_loader.obtain_order_experimental_segments(pid)\n",
    "    print(f\"Participant {pid} (ID:{data_loader.index[pid]['participant_id']}) had the experimental sequence: {video_sequence}\")\n",
    "    \n",
    "    for affect_segment in video_sequence:\n",
    "        # Extract the starting and final timestamps for the resting stage and video stage in the \n",
    "        r_t0, r_t1, v_t0, v_t1, video_filename = data_loader.calculate_info_from_segment(pid, affect_segment)\n",
    "\n",
    "        duration_rest = r_t1-r_t0\n",
    "        duration_video = v_t1-v_t0\n",
    "\n",
    "        # Show a warning if the data is shorter than expected\n",
    "        if(duration_rest < 115):\n",
    "            print(f\"Short data in REST stage!!: Participant {pid} and segment {affect_segment} ({video_filename})\")\n",
    "        if(duration_video < 295):\n",
    "            print(f\"Short data in VIDEO stage!!: Participant {pid} and segment {affect_segment} ({video_filename})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pipeline to combine physiology and affective annotations**\n",
    "\n",
    "Finally, we provide an example to generate a subset of the dataset with the following preprocessing stages:\n",
    "\n",
    "1. Identify the timestamps for the resting stage $[r_{t0},r_{t1}]$ and the stage watching the video $[v_{t0},v_{t1}]$\n",
    "2. Resample the dataframes at 50Hz\n",
    "3. Find the affective ratings and videoID of the content being watched at each moment (facilitates filtering per video, if desired)\n",
    "4. Merge the physiological and emotional data with corresponding timestamps.\n",
    "5. Merge data from all participants in a CSV file\n",
    "\n",
    "First, we present the pipeline for a single participant, and then **merge** all datasets in a single exported `.csv` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data from a specific stage.\n",
    "The function `load_data_from_affect_segment()` summarizes the process of getting individual rest and video data for a given affect stage. As shown in the previous code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rest, data_video = data_loader.load_data_from_affect_segment(PARTICIPANT_IDX, AFFECTIVE_SEGMENT, columns=COLNAMES_PHYSIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_rest.shape, data_video.shape)\n",
    "print(\"Duration stage REST: \", data_rest.index[-1] - data_rest.index[0])\n",
    "print(\"Duration stage VIDEO: \", data_video.index[-1] - data_video.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_video.plot.line(subplots=True, figsize=(15,1*data.shape[1]), sharex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Resample data to 50Hz\n",
    "\n",
    "In order to extract features from the time series, it is common to resample the dataframes to the same sampling frequency. The function `avdosvr.analysis.dataframe_functions.resample_dataframe()` allows this process to obtain useful data from the combine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling frequency\n",
    "FS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply resampling dataframe at the defined sampling frequency.\n",
    "data_rest_resampled = resample_dataframe(data_rest, FS, keep_original_timestamps=True)\n",
    "data_video_resampled = resample_dataframe(data_video, FS, keep_original_timestamps=True)\n",
    "data_video_resampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are missing values\n",
    "data_video_resampled.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset of columns sub\n",
    "df_plot = data_video_resampled[ avdosvr.preprocessing.COLNAMES_ACCELEROMETER ]\n",
    "df_plot.plot.line(subplots=True, figsize=(8,2*df_plot.shape[1]), sharex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_video_resampled.plot.line(subplots=True, figsize=(15,1*data_video_resampled.shape[1]), sharex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment thes lines to save the image in a file\n",
    "path_to_save = gen_path_temp(f\"example_figure\", extension=\".png\")\n",
    "data_video_resampled.plot.line(subplots=True, figsize=(15,1*data_video_resampled.shape[1]), sharex=True)[0].figure.savefig(path_to_save)\n",
    "print(path_to_save)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find affect states and Video IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the subjective affective ratings corresponding to the file of a specific `affect segment`. The function `load_emotions_from_affect_segment()` obtains the individual rest and video affect ratings for a given affect stage. The returned values are:\n",
    "\n",
    "- `Valence`: Affective valence rating. Range 1-9\n",
    "- `Arousal`: Affective arousal rating. Range 1-9\n",
    "- `RawX`: Raw input x-axis from joystick used to report valence. Range 0-255\n",
    "- `RawY`: Raw input y-axis from joystick used to report arousal. Range 0-255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_rest, emotions_video = data_loader.load_emotions_from_affect_segment(PARTICIPANT_IDX, AFFECTIVE_SEGMENT)\n",
    "emotions_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `calculate_video_id_end_timestamps()` provides the end time of each video within the video stage, given a user and a specific affect segment. The possible values of the column `VideoId` are:\n",
    "- `NaN`: Last timestamp of data not corresponding to a experimental segments (e.g., measuring from Emteq mask without having started the experiment)\n",
    "- `-1`: Last timestamp of the `resting video` from the specific affect segment.\n",
    "- `[int]`: Integer denoting the last timestamp of the user watching the corresponding `VideoId`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the corresponding ending of the videoID. `VideoId=-1` corresponding to a resting stage\n",
    "video_id_end_timestamp = data_loader.calculate_video_id_end_timestamps(PARTICIPANT_IDX, AFFECTIVE_SEGMENT)\n",
    "video_id_end_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merging the physiological data, affect ratings, and video ids.\n",
    "\n",
    "The sampling frequency of the physiological `data` does not match the frequency and timestamps of the `emotions`. Thus, they need to be merged.\n",
    "\n",
    "The `VideoId` corresponding to each physiological data sample can be loaded using the function `merge_asof()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the the physiological data with the emotions\n",
    "data_rest_merged = pd.merge_asof(data_rest_resampled, video_id_end_timestamp, left_index=True, right_index=True, direction=\"forward\")\n",
    "data_rest_merged.insert(0, \"OriginalParticipantID\", data_loader.index[PARTICIPANT_IDX]['participant_id'])\n",
    "data_rest_merged = pd.merge_asof(data_rest_merged, emotions_rest, left_index=True, right_index=True)\n",
    "data_rest_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge physio with affective ratings\n",
    "data_video_merged = pd.merge_asof(data_video_resampled, video_id_end_timestamp, left_index=True, right_index=True, direction=\"forward\")\n",
    "data_video_merged.insert(0, \"OriginalParticipantID\", data_loader.index[PARTICIPANT_IDX]['participant_id'])\n",
    "data_video_merged = pd.merge_asof(data_video_merged, emotions_video, left_index=True, right_index=True)\n",
    "data_video_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_compiled = data_loader.generate_merged_synchronized_dataframe(PARTICIPANT_IDX,\n",
    "                                                                AFFECTIVE_SEGMENT, \n",
    "                                                                avdosvr.preprocessing.COLNAMES_RECOMMENDED,\n",
    "                                                                sampling_frequency_hz=FS,\n",
    "                                                                set_timestamps_to_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_compiled.loc[0,\"Positive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merging all participants' data\n",
    "\n",
    "Finally, we store the postprocessed dataframes from all participants in a single CSV file. This file can be handled directly in Python because the size is much smaller than the original dataset. The sampling frequency was reduced from ~1KHz to 50Hz.\n",
    "\n",
    "In this case, we combine the raw dataset keeping two columns `[\"Participant\",\"AffectSegment\"]` to identify the individual files. However, you may involve feature extraction to a more comprehensive dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total participants\n",
    "participants_ids = data_loader.summary[\"index_id\"].unique()\n",
    "participants_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total sessions\n",
    "affect_segments = [ str(x) for x in avdosEnums.AffectSegments]\n",
    "affect_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to process per participant\n",
    "data_columns_to_analyze = avdosvr.preprocessing.COLNAMES_RECOMMENDED\n",
    "data_columns_to_analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling frequency for resampling\n",
    "SAMPLING_FREQ_HZ = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same process shown above is incorporated in the function `generate_merged_synchronized_dataframe()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â›”â›” **NOTE: The execution of the next cell may take between 30-60min because it goes through the whole dataset to generate a postprocessed version** â›”â›”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all segments for all participants and store the resting and video parts in a single large CSV.\n",
    "\n",
    "DATASET_POSTPROCESSED_FILENAME = gen_path_temp(\"Dataset_AVDOSVR_postprocessed\", extension=\".csv\")\n",
    "\n",
    "output_filename = DATASET_POSTPROCESSED_FILENAME\n",
    "\n",
    "# Variable to store the final dataset\n",
    "dataset_postprocessed_final = None\n",
    "# Check if file already exists\n",
    "if (os.path.isfile(output_filename)):\n",
    "    dataset_postprocessed_final = pd.read_csv(output_filename)\n",
    "    print(f\"File loaded from path!\")\n",
    "# Otherwise generate it\n",
    "else:\n",
    "    print(f\"Generating file!\")\n",
    "    for participant in participants_ids:\n",
    "        for aff_segment in affect_segments:\n",
    "            print(f\"\\n\\nAnalyzing participant {participant} segment {aff_segment}\")\n",
    "\n",
    "            # Final concatenation of resting and video stages\n",
    "            data_compiled = data_loader.generate_merged_synchronized_dataframe(participant,\n",
    "                                                                                 aff_segment, \n",
    "                                                                                 data_columns_to_analyze,\n",
    "                                                                                 sampling_frequency_hz=SAMPLING_FREQ_HZ,\n",
    "                                                                                 set_timestamps_to_zero=True)\n",
    "\n",
    "            # Generate final DF\n",
    "            if(dataset_postprocessed_final is None):\n",
    "                dataset_postprocessed_final = data_compiled.copy(deep=True)\n",
    "            else:\n",
    "                dataset_postprocessed_final = pd.concat([dataset_postprocessed_final, data_compiled.copy(deep=True)])\n",
    "\n",
    "        # Saving .csv every iteration\n",
    "        dataset_postprocessed_final.to_csv( output_filename )\n",
    "    print(\"\\n\\n End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_postprocessed_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_postprocessed_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">> FINISHED WITHOUT ERRORS!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8be33a246b23b79b36555b26872bcac753cc5311773880d7b4abb5b9e455248"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
