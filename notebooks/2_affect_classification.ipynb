{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AVDOS-VR` - Virtual Reality Affective Video Database with Physiological Signals\n",
    "\n",
    "Check `1_preprocess...ipynb` to see details on how to use the scripts to generate a preprocessed dataset compatible with this notebook.\n",
    "\n",
    "This notebook takes a single postprocessed file `Dataset_AVDOSVR_full_postprocessed.csv` to generate the statistical analysis and feature-based classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add files to sys.path\n",
    "from pathlib import Path\n",
    "import sys,os\n",
    "this_path = None\n",
    "try:    # WORKS WITH .py\n",
    "    this_path = str(os.path.dirname(os.path.abspath(__file__)))\n",
    "except: # WORKS WITH .ipynb\n",
    "    this_path = str(Path().absolute())+\"/\" \n",
    "print(\"File Path:\", this_path)\n",
    "\n",
    "# Add the level up to the file path so it recognizes the scripts inside `avdosvr`\n",
    "sys.path.append(os.path.join(this_path, \"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classes\n",
    "import avdosvr.preprocessing       # Generate dataset index, load files, and plots.\n",
    "\n",
    "# Utils for generation of files and paths\n",
    "from avdosvr.utils import files_handler\n",
    "\n",
    "# Import data science libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib.rcParams['text.usetex'] = True\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical tests\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import mannwhitneyu, f_oneway\n",
    "\n",
    "# Preprocessing\n",
    "import neurokit2 as nk\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature based classification\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import Lasso, RidgeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "# from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "# from lime import lime_tabular\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "# Turn off chained assignment warning\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "Global variables and functions for file management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General configuration\n",
    "\n",
    "# Path to the participants' folder w.r.t this notebook's filepath\n",
    "DATASET_ROOT_FOLDER = \"../data/\"\n",
    "\n",
    "# Used to generate the path of temporary subfolders\n",
    "NOTEBOOK_NAME = \"2_affect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to generate filepaths\n",
    "\n",
    "# MAIN FOLDERS FOR OUTPUT FILES\n",
    "ROOT = this_path + \"\"   # Root folder for all the files w.r.t this file\n",
    "TEMP_FOLDER = ROOT+\"temp/\"  # Main folder for temp files with intermediate calculations\n",
    "RESULTS_FOLDER = ROOT+\"results/\"    # Folder to recreate plots and results from analyses\n",
    "\n",
    "EXPORT_PLOTS = True\n",
    "IMG_FORMAT = \".pdf\"\n",
    "\n",
    "# Generates paths for files created from this script\n",
    "\n",
    "def gen_path_plot(filename, extension=IMG_FORMAT):\n",
    "    # Generates full paths for PLOTS just by specifying a name\n",
    "    return files_handler.generate_complete_path(filename, \\\n",
    "                                        main_folder=RESULTS_FOLDER, \\\n",
    "                                        subfolders=NOTEBOOK_NAME+\"/plots/\", \\\n",
    "                                        file_extension=extension, save_files=EXPORT_PLOTS)\n",
    "\n",
    "def gen_path_temp(filename, extension, subfolders=\"\"):\n",
    "    # Generates full paths for TEMP FILES just by specifying a name\n",
    "    return files_handler.generate_complete_path(filename, \\\n",
    "                                        main_folder=TEMP_FOLDER, \\\n",
    "                                        subfolders=NOTEBOOK_NAME+\"/\"+subfolders, \\\n",
    "                                        file_extension=extension)\n",
    "\n",
    "def gen_path_results(filename, subfolders=\"\", extension=\"\"):\n",
    "    # Generates full paths for RESULTS FILES (like pandas dataframes)\n",
    "    return files_handler.generate_complete_path(filename, \\\n",
    "                                        main_folder=RESULTS_FOLDER, \\\n",
    "                                        subfolders=NOTEBOOK_NAME+\"/\"+subfolders, \\\n",
    "                                        file_extension=extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Analysis 1: Validation of Subjective Self-reported Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Valence-Arousal ratings per video across all data from participants\n",
    "\n",
    "*Conclusion from statistical tests and plot:*\n",
    "- It shows how the video categories: `Negative` and `Positive` can be discriminated from the Valence component, but not from the arousal. As expected.\n",
    "- The categories in Valence may be used as ground-truth for the ML task. But not the presumed labels in Arousal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The preprocessing manager analyzes the original data folder\n",
    "# to create an index and facilitate preprocessing.\n",
    "data_loader = avdosvr.preprocessing.Manager(DATASET_ROOT_FOLDER, index_files_path = TEMP_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total participants\n",
    "participants_ids = data_loader.summary[\"index_id\"].unique()\n",
    "participants_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total sessions\n",
    "experiment_segment_names = data_loader.summary[\"Segment\"].unique()\n",
    "experiment_segment_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Testing merge between reported emotions and experimental segment\n",
    "# PID = 29 # Corresponds to participant_360_v2\n",
    "# df_em = data_loader.emotions[PID]\n",
    "# df_seg = data_loader.segments[PID].drop(\"Session\",axis=1) # Session is duplicated\n",
    "# df_seg = df_seg[ df_seg[\"Trigger\"] != \"Start\"] # To avoid errors when start and end are intertwined\n",
    "# df = pd.merge_asof(df_em, df_seg, left_index=True, right_index=True, direction=\"forward\")\n",
    "# df[\"VideoId\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the videoIds to each of the unaltered affective ratings\n",
    "affect_ratings_matched_video_id = None\n",
    "for participant in participants_ids:\n",
    "    # Preparing dataframes to be merged\n",
    "    df_em = data_loader.emotions[participant]\n",
    "    df_seg = data_loader.segments[participant].drop(\"Session\",axis=1) # Session is duplicated\n",
    "    df_seg = df_seg[ df_seg[\"Trigger\"] != \"Start\"] # To avoid errors when start and end are intertwined\n",
    "    # Merging\n",
    "    this_affect_ratings_merged = pd.merge_asof(df_em, df_seg, left_index=True, right_index=True, direction=\"forward\")\n",
    "    this_affect_ratings_merged.insert(0,\"p_index_id\",participant)\n",
    "    this_affect_ratings_merged.insert(1,\"participant_id\",int(data_loader.index[participant][\"participant_id\"]))\n",
    "    \n",
    "    affect_ratings_matched_video_id = this_affect_ratings_merged if (affect_ratings_matched_video_id is None) else \\\n",
    "                                        pd.concat([affect_ratings_matched_video_id,this_affect_ratings_merged], axis=0, ignore_index=True)\n",
    "\n",
    "# Show result\n",
    "MATCHED_AFFECTIVE_RATINGS_FILENAME = gen_path_temp(\"AffectiveRatingsMatchedVideoId\",extension=\".csv\")\n",
    "affect_ratings_matched_video_id.to_csv( MATCHED_AFFECTIVE_RATINGS_FILENAME, index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affect_ratings_matched_video_id.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows with `VideoId=NaN` mean that an affective ratings was generated outside the valid experimental segments. Valid segments are within the 120s-long resting stage or the 300s-long videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affect_ratings_matched_video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affect_ratings_matched_video_id.dropna(axis=0, subset=[\"VideoId\"], inplace=True)\n",
    "affect_ratings_matched_video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the data from affect segments (excluding `video_1` and `video_5`) corresponding to resting stages.\n",
    "Q = ( (affect_ratings_matched_video_id[\"Segment\"]==\"Positive\") | \\\n",
    "        (affect_ratings_matched_video_id[\"Segment\"]==\"Negative\") | \\\n",
    "        (affect_ratings_matched_video_id[\"Segment\"]==\"Neutral\") | \\\n",
    "        (affect_ratings_matched_video_id[\"Segment\"]==\"video_5\"))\n",
    "affect_ratings_matched_video_id = affect_ratings_matched_video_id[ Q ]\n",
    "affect_ratings_matched_video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the resting videoId with text for plot\n",
    "affect_ratings_matched_video_id = affect_ratings_matched_video_id.astype({\"VideoId\":int})\n",
    "# Resting in positive segment\n",
    "Q = ( (affect_ratings_matched_video_id[\"Segment\"]==\"Positive\") & (affect_ratings_matched_video_id[\"VideoId\"]==-1) )\n",
    "affect_ratings_matched_video_id.loc[Q,\"VideoId\"]=\"R+\"\n",
    "# Resting in positive segment\n",
    "Q = ( (affect_ratings_matched_video_id[\"Segment\"]==\"Neutral\") & (affect_ratings_matched_video_id[\"VideoId\"]==-1) )\n",
    "affect_ratings_matched_video_id.loc[Q,\"VideoId\"]=\"Rn\"\n",
    "# Resting in positive segment\n",
    "Q = ( (affect_ratings_matched_video_id[\"Segment\"]==\"Negative\") & (affect_ratings_matched_video_id[\"VideoId\"]==-1) )\n",
    "affect_ratings_matched_video_id.loc[Q,\"VideoId\"]=\"R-\"\n",
    "# Resting in final end segment\n",
    "Q = ( (affect_ratings_matched_video_id[\"Segment\"]==\"video_5\") & (affect_ratings_matched_video_id[\"VideoId\"]==-1) )\n",
    "affect_ratings_matched_video_id.loc[Q,\"VideoId\"]=\"R-end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and save total and mean number of ratings per video\n",
    "NUMBER_OF_RATINGS_FILENAME = gen_path_results(\"2_NumberOfRatingsPerVideoID\", extension=\".csv\")\n",
    "df_number_of_ratings = pd.DataFrame(affect_ratings_matched_video_id[\"VideoId\"].value_counts().index.array,columns=['VideoId'])\n",
    "df_number_of_ratings['TotalRatings'] = affect_ratings_matched_video_id[\"VideoId\"].value_counts().values\n",
    "df_number_of_ratings['MeanRatings'] = df_number_of_ratings['TotalRatings']/len(participants_ids)\n",
    "#df_number_of_ratings = df_number_of_ratings.sort_values(by=['TotalRatings'],ascending=False).reset_index(drop=True)\n",
    "df_number_of_ratings.to_csv( NUMBER_OF_RATINGS_FILENAME, index=False)\n",
    "df_number_of_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Number of ratings per videoID\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,2,2])\n",
    "ax.bar(df_number_of_ratings['VideoId'].astype(str),df_number_of_ratings['TotalRatings'].values)\n",
    "plt.xlabel('Video ID') \n",
    "plt.ylabel('Total Ratings') \n",
    "plt.title(\"Combined ratings from all participants for each video\")\n",
    "save_path_plot = gen_path_plot(\"combined-total-ratings-per-video\")\n",
    "plt.savefig(save_path_plot,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean number of ratings per videoID\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,2,2])\n",
    "ax.bar(df_number_of_ratings['VideoId'].astype(str),df_number_of_ratings['MeanRatings'].values)\n",
    "plt.xlabel('Video ID') \n",
    "plt.ylabel('Mean Ratings') \n",
    "plt.title(\"Mean number of ratings for each video\")\n",
    "save_path_plot = gen_path_plot(\"mean-number-of-ratings-per-video\")\n",
    "plt.savefig(save_path_plot,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average ratings per video\n",
    "df_results_avg_affect_per_video = affect_ratings_matched_video_id.groupby(['p_index_id','VideoId']).mean().reset_index()\n",
    "df_results_avg_affect_per_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_video_segment = affect_ratings_matched_video_id.groupby(\"VideoId\").first()[\"Segment\"]\n",
    "matching_video_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_aff = df_results_avg_affect_per_video.groupby([\"VideoId\"]).mean()[[\"Valence\",\"Arousal\"]].join(matching_video_segment).reset_index()\n",
    "avg_aff[\"Segment\"].iloc[ avg_aff[\"VideoId\"].str.startswith(\"R\").replace(np.nan, False) ] = \"Rest\"\n",
    "avg_aff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot_affect_coordinates_ratings(df, ax):\n",
    "\n",
    "    av_aff = df.copy()\n",
    "    kwargs = {\"Negative\":{\n",
    "                            \"c\":\"darkorange\", \"marker\":\"s\"\n",
    "                            },\n",
    "                    \"Positive\":{\n",
    "                            \"c\":\"forestgreen\", \"marker\":\"D\"\n",
    "                        },\n",
    "                    \"Neutral\":{\n",
    "                            \"c\":\"grey\", \"marker\":\"^\"\n",
    "                        },\n",
    "                    \"Rest\":{\n",
    "                            \"c\":\"dodgerblue\", \"marker\":\"o\", \"edgecolor\":\"k\", \"linewidths\":1\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "    # colors = [kwargs[val]['c'] for val in avg_aff[\"Segment\"]]\n",
    "    # markers = [kwargs[val]['marker'] for val in avg_aff[\"Segment\"]]\n",
    "    # avg_aff.plot.scatter(ax=ax, x=\"Valence\",y=\"Arousal\",color=colors, markers=markers, legend=True)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    # Plot\n",
    "    offset_min = 2.5\n",
    "    offset_max = 7.5\n",
    "    ax.set(xlim=[offset_min,offset_max], ylim=[offset_min,offset_max])\n",
    "    ax.set_xlabel(\"Valence\")\n",
    "    ax.set_ylabel(\"Arousal\")\n",
    "    # ax.set_title(f\"Average Perceived Valence-Arousal ratings per video\")\n",
    "\n",
    "    ax.vlines([(offset_max+offset_min)/2], ymin=offset_min, ymax=offset_max, color=\"gray\", linestyle='dashed', linewidth=2)\n",
    "    ax.hlines([(offset_max+offset_min)/2], xmin=offset_min, xmax=offset_max, color=\"gray\", linestyle='dashed', linewidth=2)\n",
    "    ax.grid(True)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    for segment in av_aff[\"Segment\"].unique():\n",
    "        df_test = av_aff[ (av_aff[\"Segment\"] == segment)]\n",
    "        avg_V = df_test[\"Valence\"] #((df_test.sum_RawX/df_test.N)-128)/256 # Normalized affect between 0-1\n",
    "        avg_A = df_test[\"Arousal\"] #((df_test.sum_RawY/df_test.N)-128)/256\n",
    "\n",
    "        ax.scatter( avg_V, avg_A, label=segment, s=30, **kwargs[segment] ) #c=colors[segment], s=80, marker=markers[segment])\n",
    "\n",
    "        for i, videoId in enumerate(df_test[\"VideoId\"].values):\n",
    "            avg_V = df_test[\"Valence\"]\n",
    "            avg_A = df_test[\"Arousal\"]\n",
    "\n",
    "            # Defines where to offset the annotations depending on how many neighbors are there\n",
    "            offsetX = 0.2\n",
    "            offsetY = 0.2\n",
    "\n",
    "            offset_dict = {\n",
    "                0: [0, -offsetY],\n",
    "                1: [0, offsetY],\n",
    "                2: [-offsetX, 0],\n",
    "                3: [offsetX, 0]\n",
    "            }\n",
    "            # Define offset based on how many points are close to the value\n",
    "            # np.random.seed(345)\n",
    "            # offset_annotation = offset_dict[np.floor(np.random.randint(4))] #offset_dict[0] #offset_dict[np.floor(np.random.randint(4))]\n",
    "            thresh = 0.2\n",
    "            radius = np.sqrt(np.abs(avg_A.iloc[i] - avg_A.iloc[:i])**2 + (np.abs(avg_V.iloc[i] - avg_V.iloc[:i])**2))\n",
    "            n_close_neighbors = (radius<thresh).sum()\n",
    "            offset_annotation = offset_dict[n_close_neighbors]\n",
    "\n",
    "            # Annotate\n",
    "            if(segment != \"Rest\"):\n",
    "                ax.annotate(videoId, (avg_V.iloc[i]+offset_annotation[0], \n",
    "                                    avg_A.iloc[i]+offset_annotation[1]), \n",
    "                                    fontsize=9, ha='center', va='center',\n",
    "                                    color='k')  #kwargs[segment][\"c\"])\n",
    "    \n",
    "    ax.grid(True)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    return ax\n",
    "\n",
    "# Plotting how each video was rated among all self-reported datapoints across all 15 participants.\n",
    "save_path_plot = gen_path_plot(\"affect-coordinates-ratings-per-video\")\n",
    "\n",
    "NUM_ROWS = 1\n",
    "NUM_COLS = 1\n",
    "fig,axes = plt.subplots(NUM_ROWS, NUM_COLS, sharex=False, sharey=True, figsize=(6*NUM_COLS, 4*NUM_ROWS))\n",
    "\n",
    "axes = generate_plot_affect_coordinates_ratings(avg_aff, axes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical analysis tests\n",
    "\n",
    "Validate whether the average self-reported **valence** and **arousal** ratings differ between video categories aiming to induce `Negative`, `Neutral`, and `Positive` affect.\n",
    "\n",
    "**Method:** Paired t-test to compare the mean of two samples.\n",
    "\n",
    "*Test 1*: $H_0: \\mu_- \\geq \\mu_N$ | $H_1: \\mu_- < \\mu_N$\n",
    "\n",
    "If $p<0.01$, we reject the null hypothesis that the mean **reported valence/arousal** in the `Negative` videos $\\mu_-$ is *greater or equal*  than in the `Neutral` videos $\\mu_N$\n",
    "\n",
    "*Test 2*: $H_0: \\mu_+ \\leq \\mu_N$ | $H_1: \\mu_+ > \\mu_N$\n",
    "\n",
    "If $p<0.01$, we reject the null hypothesis that the mean **reported valence/arousal** in the `Positive` videos $\\mu_-$ is *lower or equal* than in the `Neutral` videos $\\mu_N$\n",
    "\n",
    "\n",
    "**Conclusions**\n",
    "\n",
    "Regarding *valence*, the mean ratings in the negative videos are significantly lower than the ratings in the neutral videos ($T=-10.96;N=10;p<0.001$). Similarly, the mean *valence* ratings in the positive videos are greater than in the neutral ($T=6.62;N=10;p<0.001$) as well as the mean valence ratings being signfiicantly higher in positive than negative category ($T=19.73;N=10;p<0.001$). \n",
    "\n",
    "Regarding *arousal*, the mean ratings in the negative videos are greater than in the neutral ($T=9.58;N=10;p<0.001$). Similarly, the mean arousal ratings in positive category are greater than in neutral ($T=9.94;N=10;p<0.001$). Finally, as expected, since both negative and positive videos induced high arousal, the mean arousal ratings in these two categories is not significantly different ($T=-0.89;N=10;p<0.8$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affect_ratings_matched_video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace columns as categorical to get all factors when grouping by\n",
    "X = affect_ratings_matched_video_id.copy()\n",
    "# X[\"p_index_id\"] = pd.Categorical(X[\"p_index_id\"], categories=X[\"p_index_id\"].unique(), ordered=False)\n",
    "# X[\"participant_id\"] = pd.Categorical(X[\"participant_id\"].astype(int), categories=X[\"participant_id\"].unique().astype(int), ordered=False)\n",
    "X[\"VideoId\"] = pd.Categorical(X[\"VideoId\"], categories=X[\"VideoId\"].unique(), ordered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate DF with all participants and all videoIds (even if they have no ratings)\n",
    "# result should be an array of 39 participants * 3 affect segments * (10+1) videos per segment = 1287\n",
    "df_avg_per_participant_and_video = X.groupby([\"p_index_id\",\"VideoId\"]).mean().reset_index().set_index([\"VideoId\"])\n",
    "df_avg_per_participant_and_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a DF with summary affect ratings across participants, and join with corresponding affect segment\n",
    "df_statistical_comparison = df_avg_per_participant_and_video.groupby(\"VideoId\").mean().join(matching_video_segment).reset_index()\n",
    "#Drop participant ID columns as the resulting df is average across all participants\n",
    "df_statistical_comparison = df_statistical_comparison.drop(columns=['p_index_id', 'participant_id'])\n",
    "df_statistical_comparison[\"stage\"] = \"video\"\n",
    "df_statistical_comparison[\"stage\"].iloc[ df_statistical_comparison[\"VideoId\"].str.startswith(\"R\").replace(np.nan, False) ] = \"rest\"\n",
    "\n",
    "# Save file\n",
    "STATISTICAL_TESTS_FILENAME = gen_path_results(\"2_AvgAffectRatingsPerVideoId\", extension=\".csv\")\n",
    "df_statistical_comparison.to_csv( STATISTICAL_TESTS_FILENAME, index=False)\n",
    "\n",
    "df_statistical_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Valence**\n",
    "\n",
    "We analyze `RawX` instead of `Valence` because it contains the raw reported values from the joystick and has not been quantisized in the `9-level` variable of valence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just use the data from the videos, not the resting stages\n",
    "df_comparisons = df_statistical_comparison[ df_statistical_comparison[\"stage\"]==\"video\" ].sort_values(by=['VideoId'])\n",
    "df_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate columns for statistical test\n",
    "subj_valence_videos_neg = df_comparisons[df_comparisons[\"Segment\"]==\"Negative\"][\"RawX\"]\n",
    "subj_valence_videos_ntr = df_comparisons[df_comparisons[\"Segment\"]==\"Neutral\"][\"RawX\"]\n",
    "subj_valence_videos_pos = df_comparisons[df_comparisons[\"Segment\"]==\"Positive\"][\"RawX\"]\n",
    "\n",
    "print(f\"N = {subj_valence_videos_neg.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Negative: \\t{subj_valence_videos_neg.mean()} +/- ({subj_valence_videos_neg.std()})\")\n",
    "print(f\"Neutral: \\t{subj_valence_videos_ntr.mean()} +/- ({subj_valence_videos_ntr.std()})\")\n",
    "print(f\"Positive: \\t{subj_valence_videos_pos.mean()} +/- ({subj_valence_videos_pos.std()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "negative < neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical test: paired t-test\n",
    "test1 = stats.ttest_rel(subj_valence_videos_neg, subj_valence_videos_ntr, alternative=\"less\")\n",
    "print(test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pos > neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = stats.ttest_rel(subj_valence_videos_pos, subj_valence_videos_ntr, alternative=\"greater\")\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pos > negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = stats.ttest_rel(subj_valence_videos_pos, subj_valence_videos_neg, alternative=\"greater\")\n",
    "print(test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_text = gen_path_results(\"2_Affect_Valence_Paired-T-Tests\", extension=\".txt\")\n",
    "f = open(save_path_text, \"w\")\n",
    "f.write(\"Test 1: (H1) Negative < than Neutral:\\t\" + str(test1) + \"\\n\")\n",
    "f.write(\"Test 2: (H1) Positive > than Neutral:\\t\" + str(test2) + \"\\n\")\n",
    "f.write(\"Test 3: (H1) Positive > than Negative:\\t\" + str(test3) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arousal**\n",
    "\n",
    "Analyzing average value from `RawY` from the joystick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate columns for statistical test\n",
    "subj_arousal_videos_neg = df_comparisons[df_comparisons[\"Segment\"]==\"Negative\"][\"RawY\"]\n",
    "subj_arousal_videos_ntr = df_comparisons[df_comparisons[\"Segment\"]==\"Neutral\"][\"RawY\"]\n",
    "subj_arousal_videos_pos = df_comparisons[df_comparisons[\"Segment\"]==\"Positive\"][\"RawY\"]\n",
    "\n",
    "print(f\"Negative: \\t{subj_arousal_videos_neg.mean()} +/- ({subj_arousal_videos_neg.std()})\")\n",
    "print(f\"Neutral: \\t{subj_arousal_videos_ntr.mean()} +/- ({subj_arousal_videos_ntr.std()})\")\n",
    "print(f\"Positive: \\t{subj_arousal_videos_pos.mean()} +/- ({subj_valence_videos_pos.std()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical test: paired t-test\n",
    "test1 = stats.ttest_rel(subj_arousal_videos_neg, subj_arousal_videos_ntr, alternative=\"greater\")\n",
    "print(test1)\n",
    "test2 = stats.ttest_rel(subj_arousal_videos_pos, subj_arousal_videos_ntr, alternative=\"greater\")\n",
    "print(test2)\n",
    "test3 = stats.ttest_rel(subj_arousal_videos_pos, subj_arousal_videos_neg, alternative=\"greater\")\n",
    "print(test3)\n",
    "\n",
    "save_path_text = gen_path_results(\"_Affect_Arousal_Paired-T-Tests\", extension=\".txt\")\n",
    "f = open(save_path_text, \"w\")\n",
    "f.write(\"Test 1: (H1) Negative > than Neutral:\\t\" + str(test1) + \"\\n\")\n",
    "f.write(\"Test 2: (H1) Positive > than Neutral:\\t\" + str(test2) + \"\\n\")\n",
    "f.write(\"Test 3: (H1) Positive > than Negative:\\t\" + str(test3) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots average ratings per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_affect_with_segment = df_avg_per_participant_and_video.join(matching_video_segment).reset_index()\n",
    "# Group relaxing videos together under the same sequence for plotting\n",
    "df_avg_affect_with_segment['VideoId'] = df_avg_affect_with_segment['VideoId'].astype(str)\n",
    "df_avg_affect_with_segment['Segment'].mask(df_avg_affect_with_segment['VideoId'].str.contains('R'), 'Relax', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot_affect_boxplot_ratings(df_results, axis_to_plot, colname, title):\n",
    "    ax = axis_to_plot\n",
    "    # Sort groups\n",
    "    df_avg_affect_2 = df_results.copy()\n",
    "    df_avg_affect_2.sort_values(by=[\"VideoId\"], inplace=True, key= lambda col: col.map(lambda item: ([str,int].index(type(item)), item) ) ) # Make letters come before numbers\n",
    "    df_avg_affect_2.sort_values(by=[\"Segment\"], ascending=True, kind=\"mergesort\", inplace=True) # Mergesort keeps respecting the previous sorting decision.\n",
    "\n",
    "    # Rename columns\n",
    "    df_avg_affect_2.rename( columns = {\n",
    "                \"Segment\":\"Exp. Segment\",\n",
    "                \"VideoId\": \"Video ID\",\n",
    "                \"Valence\":\"Avg. Valence\",\n",
    "                \"Arousal\":\"Avg. Arousal\"\n",
    "                }, inplace = True )\n",
    "    \n",
    "    sns.set_theme(style=\"ticks\")\n",
    "    \n",
    "    PALETTE_COLORS = [avdosvr.preprocessing.utils.enums.colourPaletteRGB.Negative.value,\n",
    "                      avdosvr.preprocessing.utils.enums.colourPaletteRGB.Neutral.value,\n",
    "                      avdosvr.preprocessing.utils.enums.colourPaletteRGB.Positive.value,\n",
    "                      avdosvr.preprocessing.utils.enums.colourPaletteRGB.Relax.value]\n",
    "\n",
    "    # Vertical lines separating the experimental sessions\n",
    "    ax.vlines([9.5, 19.5, 29.5], ymin=1, ymax=9, color=\"black\", linestyle='dotted', linewidth=1.5)\n",
    "    ax.grid(axis=\"y\")\n",
    "\n",
    "    # Filter the data that has to do with this column label\n",
    "    sns.boxplot(data=df_avg_affect_2, ax=ax,\n",
    "                x=\"Video ID\", y=colname, hue=\"Exp. Segment\", #orient=\"h\",\n",
    "                width=0.6, palette=PALETTE_COLORS, dodge =False,\n",
    "                # notch=True, \n",
    "                showcaps=False,\n",
    "                flierprops={\"marker\": \"x\"},\n",
    "                boxprops={\"edgecolor\": \"none\"},\n",
    "                # medianprops={\"color\": \"coral\"}\n",
    "                )\n",
    "    \n",
    "    sns.despine(offset=10, trim=True)\n",
    "    sns.move_legend(ax, loc=\"best\", title=None, frameon=False)\n",
    "\n",
    "    ax.set_ylabel(f\"Affect Rating - {colname.split('.')[-1]}\", fontsize = 15)\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.set_xlabel(\"Video ID\", fontsize=15)\n",
    "    if('Arousal' in title):\n",
    "        ax.get_legend().remove()\n",
    "    return ax\n",
    "\n",
    "### Generate path to save and figure\n",
    "save_path_plot = gen_path_plot(f\"affect-boxplot-ratings-per-video\")\n",
    "\n",
    "NUM_ROWS = 1\n",
    "NUM_COLS = 2\n",
    "fig,axes = plt.subplots(NUM_ROWS, NUM_COLS, sharex=False, sharey=True, figsize=(12*NUM_COLS, 7*NUM_ROWS))\n",
    "\n",
    "COL_TITLES = [\"Avg. Valence\", \"Avg. Arousal\"]\n",
    "PLOT_TITLES = [\"Average Valence per Video\", \"Average Arousal per Video\"]\n",
    "for i in range(len(COL_TITLES)):\n",
    "    axes[i] = generate_plot_affect_boxplot_ratings(df_avg_affect_with_segment, axes[i], COL_TITLES[i], PLOT_TITLES[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analyzing postprocessed dataset and features\n",
    "\n",
    "The following blocks use compiled dataset created during the notebook `1_...ipynb`. It contains a subset from the AVDOS-VR dataset with physiological data resampled at 50Hz, subset of relevant columns, and synchronized with affect events and experimental affective segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "DATASET_POSTPROCESSED_FILENAME = \"./temp/1_preprocess/Dataset_AVDOSVR_postprocessed.csv\"\n",
    "DATASET_POSTPROCESSED_FILENAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file in memory (~700MB)\n",
    "dataset = pd.read_csv(DATASET_POSTPROCESSED_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General variables\n",
    "\n",
    "The following variables store important info about the dataset. How many participants? Which video segments are included? Which videoIds are contained in each affective segment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_ids = dataset[\"Participant\"].unique()\n",
    "participants_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_segment_names = dataset[\"Stage\"].unique()\n",
    "experiment_segment_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ids = dataset[\"VideoId\"].unique()\n",
    "print(np.sort(video_ids))\n",
    "# -1 refers to the resting stages, no video is presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All rows inside the `video` stage should contain a VideoId differen than -1,\n",
    "# As -1 denotes a resting video. The following code verifies that there are no invalid rows.\n",
    "Q = (( dataset[\"VideoId\"]==-1) & ~(dataset[\"Stage\"].str.startswith(\"Resting_\")))\n",
    "dataset[Q]\n",
    "\n",
    "# NOTE: The result should be and empty DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dictionary with the stage and their corresponding list of `VideoId`\n",
    "# Confirm the unique Video IDs in each experimental segment\n",
    "video_ids_per_segment = {}\n",
    "for segment in experiment_segment_names:\n",
    "    video_ids_per_segment[segment] = np.array([])\n",
    "    # Keep unique video ids\n",
    "    Q = ( dataset[\"Stage\"] == segment )\n",
    "    video_ids_per_segment[segment] = dataset[Q][\"VideoId\"].unique()\n",
    "video_ids_per_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The index is in the order [\"Participant\", \"AffectSegment\", \"Time\"]\n",
    "# It allows multidimensional manipulation in a 2D pandas structure\n",
    "data = dataset.set_index([\"Participant\",\"Stage\",\"Time\"])\n",
    "data.sort_index(inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - Accessing single participant's data\n",
    "Exploratory Data Analysis (EDA) to visualize a particular experimental segment from a specific participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTICIPANT_IDX = 14\n",
    "EXPERIMENTAL_SEGMENT = \"Positive\"\n",
    "# Access the dictionary of videos existing in the segment, choose the first VideoID\n",
    "VIDEO_ID = video_ids_per_segment[EXPERIMENTAL_SEGMENT][0] \n",
    "\n",
    "##\n",
    "# Select a whole experimental segment\n",
    "single_segment_ts = data.loc[(PARTICIPANT_IDX,EXPERIMENTAL_SEGMENT)]\n",
    "# Select a specific video inside the experimental segment\n",
    "single_video_ts = single_segment_ts[ single_segment_ts[\"VideoId\"] == VIDEO_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_segment_ts.plot.line(subplots=True, figsize=(15,1*single_segment_ts.shape[1]), sharex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_video_ts.plot.line(subplots=True, figsize=(5,1*single_video_ts.shape[1]), sharex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis 2: Feature-based classification\n",
    "\n",
    "Feature-based classification assuming that the features captured with overlapping windows act as independent samples, unlike in time-series classification.\n",
    "\n",
    "Feature extraction steps:\n",
    "1. Traverse the time-series data per participant and per experimental segment: (~300secs per video segment).\n",
    "2. Consider the following stages as target classes: `[Positive, Neutral, Negative]`.\n",
    "3. Take the corresponding resting stages ( `[Resting_Negative, Resting_Neutral, Resting_VideoPositive]`) to fit a standardizer ($\\mu=0,\\sigma=1$)\n",
    "4. Apply a sliding window of specific width and overlap to extract **statistical features** (11 features per dimension) from: `[HEART, MOTOR, FACE]`.\n",
    "5. In addition, calculate **time-domain HRV features** from the non-standardized version of the PPG signal (5 features from PPG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_SAMP_FREQUENCY_HZ = 50\n",
    "\n",
    "WINDOW_WIDTH_SECS = 30\n",
    "WINDOW_OVERLAP_SECS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the original dataset again\n",
    "data = dataset.copy().set_index([\"Participant\",\"Stage\",\"Time\"])\n",
    "data.sort_index(inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify groups of features\n",
    "basic_colnames = avdosvr.preprocessing.COLNAMES_AFFECT\n",
    "hrv_colnames = avdosvr.preprocessing.COLNAMES_HR + avdosvr.preprocessing.COLNAMES_PPG\n",
    "imu_colnames = avdosvr.preprocessing.COLNAMES_ACCELEROMETER + avdosvr.preprocessing.COLNAMES_MAGNETOMETER + avdosvr.preprocessing.COLNAMES_GYROSCOPE\n",
    "emg_colnames = avdosvr.preprocessing.COLNAMES_EMG_AMPLITUDE\n",
    "emg_contact_colnames = avdosvr.preprocessing.COLNAMES_EMG_CONTACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify colnames that have relevant time-series data (exclude videoId, affect, etc.)\n",
    "TS_DATA_COLNAMES = hrv_colnames + imu_colnames + emg_colnames + emg_contact_colnames\n",
    "TS_DATA_COLNAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_outliers_from_df(df, num_std = 2):\n",
    "    \"\"\"\n",
    "    Takes a multidimensional dataFrame and filter the values\n",
    "    that are `num_std` standard deviations away from the mean value\n",
    "    of the column.\n",
    "\n",
    "    First, it transforms the value in np.nan. Then, it imputes the\n",
    "    value with backward filling, and then with forward filling, in case\n",
    "    the missing values are generated on the extremes of the time-series.\n",
    "\n",
    "    Returns the filtered dataset\n",
    "    \"\"\"\n",
    "    mask = (( df > (df.mean() + num_std*df.std())) | ( df < (df.mean() - num_std*df.std())) )\n",
    "    df[ mask ] = np.nan\n",
    "    df_filtered = df.fillna(method=\"backfill\", axis=0)\n",
    "    df_filtered = df_filtered.fillna(method=\"ffill\", axis=0)\n",
    "    print(f\"\\tTotal NAs --> Generated={df.isna().sum().sum()} - After imputation={df_filtered.isna().sum().sum()}\")\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Testing feature extraction in one instance*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANCE\n",
    "PARTICIPANT_IDX = 20\n",
    "EXP_SEGMENT = str(avdosvr.preprocessing.AffectSegments.VideosNeutral)\n",
    "VIDEO_ID = video_ids_per_segment[EXP_SEGMENT][0]\n",
    "\n",
    "print(f\"participant={PARTICIPANT_IDX}, segment={EXP_SEGMENT}, video_id={VIDEO_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing resting stage as baseline for the scaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from dataset data from a VIDEO segment and its corresponding RESTING stage\n",
    "single_segment_ts = data.loc[(PARTICIPANT_IDX,EXP_SEGMENT)]\n",
    "\n",
    "# Filter data with FitState >= average (8)\n",
    "single_segment_ts = single_segment_ts[ single_segment_ts[\"Faceplate/FitState\"] > 7]\n",
    "single_segment_ts = single_segment_ts[TS_DATA_COLNAMES] # Choose columns with relevant data\n",
    "\n",
    "# This resting stages will be used to normalize the values.\n",
    "single_segment_resting = data.loc[(PARTICIPANT_IDX,\"Resting_\"+EXP_SEGMENT)]\n",
    "# Filter data with FitState >= average (8)\n",
    "single_segment_resting = single_segment_resting[ single_segment_resting[\"Faceplate/FitState\"] > 7]\n",
    "\n",
    "print(f\"Video df shape: {single_segment_ts.shape}\")\n",
    "print(f\"Resting df shape: {single_segment_resting.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data from each participant used for standarisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data from a participant across all experiments\n",
    "single_participants_data = data.loc[(PARTICIPANT_IDX)]\n",
    "single_participants_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that each column has a mean different than zero and std different than one\n",
    "single_participants_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler fitted on the WHOLE data of the participant\n",
    "scaler = ColumnTransformer( [ \n",
    "                            (\"\", StandardScaler(), TS_DATA_COLNAMES) # Apply to all columns with TS data\n",
    "                        ])\n",
    "scaler.fit(single_participants_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proof: The standardization creates the z-scores \n",
    "# trained above and each column has zero mean and one std\n",
    "scaled_data = scaler.transform(single_participants_data)\n",
    "scaled_data = pd.DataFrame(data=scaled_data,columns=TS_DATA_COLNAMES)\n",
    "scaled_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sliding window to iterate over the signal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_windows(df_ts, \n",
    "                        window_width_seconds,\n",
    "                        window_overlap_seconds,\n",
    "                        verbose = False\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    Iterates over the dataframe in `df_ts` with overlapping windows\n",
    "    defined by `window_width_seconds` and `window_overlap_seconds`.\n",
    "\n",
    "    This function assumes that the index of `df_ts` is in seconds\n",
    "    and is sorted incrementally.\n",
    "    Returns a list of tuples with the (start,end) of each window\n",
    "    \"\"\"\n",
    "    # Generate iterator\n",
    "    w_start_indices = np.arange(df_ts.index[0], df_ts.index[-1], window_overlap_seconds)\n",
    "    return [ (w, w+window_width_seconds) for w in w_start_indices ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sliding_windows(single_segment_ts,\n",
    "                        window_overlap_seconds = WINDOW_OVERLAP_SECS,\n",
    "                        window_width_seconds = WINDOW_WIDTH_SECS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first window\n",
    "df_window = single_segment_ts.loc[0:WINDOW_WIDTH_SECS]\n",
    "df_window.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract HRV features from PPG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hrv_features(df_ppg, sampling_frequency, return_plot=False):\n",
    "    \"\"\"\n",
    "    Receives a dataframe with ppg data and returns\n",
    "    time-domain features.\n",
    "    \"\"\"\n",
    "    \n",
    "    signals, info = nk.ppg_process(df_ppg, sampling_rate=sampling_frequency)\n",
    "    peaks = signals.PPG_Peaks\n",
    "    \n",
    "    ## Plot summary\n",
    "    # nk.ppg_plot(signals, sampling_rate=ORIG_SAMP_FREQUENCY_HZ)\n",
    "\n",
    "    # Time-based features\n",
    "    hrv_time = nk.hrv_time(peaks, sampling_rate=sampling_frequency, show=False)\n",
    "\n",
    "    # HRV features from neurokit2 that should be forwarded for final dataset\n",
    "    HRV_SUBSET_FEATURES = [\"HRV_MeanNN\",\"HRV_SDNN\",\"HRV_RMSSD\",\"HRV_SDSD\",\"HRV_MedianNN\", \"HRV_IQRNN\"]\n",
    "    hrv_time_features = hrv_time[ HRV_SUBSET_FEATURES ]\n",
    "    \n",
    "    # Calculate mean heart rate for the window\n",
    "    hrv_time_features['Mean_BPM'] = np.mean(signals['PPG_Rate'])\n",
    "    \n",
    "    # Get values for actual peaks\n",
    "    ppg_peaks = signals[signals['PPG_Peaks']==1]\n",
    "    ppg_peaks = ppg_peaks.reset_index()\n",
    "    \n",
    "    for idx, peak in ppg_peaks.iterrows():\n",
    "        #get number of rows between PPG peaks\n",
    "        if(idx < (len(ppg_peaks)-2)):\n",
    "            time_between_peaks = ppg_peaks.loc[idx+1]['index'] - ppg_peaks.loc[idx]['index']\n",
    "            #with known frequency of 50hz, get seconds between peaks\n",
    "            time_between_peaks = time_between_peaks * 0.02\n",
    "            # estimate BPM with the time between peaks\n",
    "            estimated_hr = 60/time_between_peaks\n",
    "            # check if BPM estimation falls outside the 40 to 120 bpm range\n",
    "            if ((estimated_hr < 40) or (estimated_hr > 120)):\n",
    "                hrv_time_features[hrv_time_features.columns] = -1\n",
    "                break\n",
    "\n",
    "    # ## NOTE: Frequency features are not used because the window width may not enough for most features!\n",
    "    # hrv_freq = nk.hrv_frequency(peaks, sampling_rate=ts_sampling_freq, show=True, normalize=True)\n",
    "    # hrv_allfeatures = nk.hrv(peaks, sampling_rate=ts_sampling_freq, show=True)\n",
    "\n",
    "    #### Save figure when `show=True`\n",
    "    # save_path_plot = gen_path_plot(f\"Preprocessing/PPG/Participant{participant}_{segment}\")\n",
    "    # fig = plt.gcf().set_size_inches(8, 5)\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(save_path_plot)\n",
    "    # plt.close()\n",
    "    return hrv_time_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPG_COLNAME = 'Ppg/Raw.ppg'     \n",
    "calculate_hrv_features(df_window[PPG_COLNAME], ORIG_SAMP_FREQUENCY_HZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract statistical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistical_features(df):\n",
    "    \"\"\"\n",
    "    Calculates the following features per column in the dataframe,\n",
    "    adding a suffix for each processed column:\n",
    "        - mean:     mean\n",
    "        - std:      standard deviation\n",
    "        - min:      minimum value\n",
    "        - max:      maximum value\n",
    "        - median:   median\n",
    "        - irq:      interquartile range\n",
    "        - pnv:      proportion of negative values\n",
    "        - ppv:      proportion of positive values\n",
    "        - skew:     skewness of the distribution\n",
    "        - kurt:     kurtosis of the distribution\n",
    "        - energy:   sum of squared absolute values\n",
    "        - rms:      sqrt(sum of squared absolute values/n) \n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = pd.DataFrame(df)\n",
    "\n",
    "    FUNCTIONS_FEATURES = {\n",
    "        \"mean\":     np.mean,\n",
    "        \"std\":      np.std,\n",
    "        \"min\":      np.min,\n",
    "        \"max\":      np.max,\n",
    "        \"median\":   np.median,\n",
    "        \"irq\":      stats.iqr,\n",
    "        \"pnv\":      (lambda y: y[y<0].size/y.size),\n",
    "        \"ppv\":      (lambda y: y[y>0].size/y.size),\n",
    "        \"skew\":     stats.skew,\n",
    "        \"kurt\":     stats.kurtosis,\n",
    "        \"energy\":   (lambda y: np.sum(np.abs(y)**2) ),\n",
    "        \"rms\":      (lambda y: np.sqrt(np.sum(np.abs(y)**2)/y.size) ),\n",
    "    }\n",
    "    \n",
    "    # Store results with features per columns\n",
    "    df_features_results = { }\n",
    "\n",
    "    for feat_name,feat_func in FUNCTIONS_FEATURES.items():\n",
    "        for col_name in list(df.columns):\n",
    "            df_features_results[f\"{col_name}_{feat_name}\"] = [ feat_func(df[col_name]) ]\n",
    "\n",
    "    return pd.DataFrame(df_features_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standardization on the VIDEO experimental stage\n",
    "# Normalize the data from each participant w.r.t to the same variable during the corresponding resting stage.\n",
    "df_window_norm = pd.DataFrame(data = scaler.transform(df_window), \n",
    "                                                columns = TS_DATA_COLNAMES, \n",
    "                                                index = df_window.index)\n",
    "\n",
    "\"\"\" Create statistical feature \"\"\"\n",
    "statistical_features = calculate_statistical_features(df_window_norm)\n",
    "statistical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistical_features.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_ts_window(data_df_window, ts_sampling_freq:float,\n",
    "                                    colname_ppg:str,\n",
    "                                    fitted_scaler:object,\n",
    "                                    columns_to_normalize:list):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and extracts HRV and summary features from it\n",
    "    Returns a dataframe with one row combining all features.\n",
    "    \"\"\"\n",
    "    df_window = data_df_window\n",
    "    \n",
    "    # Extract HRV features from PPG\n",
    "    hrv_time_features = calculate_hrv_features(df_window[colname_ppg], ts_sampling_freq)\n",
    "    \n",
    "    \"\"\" Create statistical feature \"\"\"\n",
    "    # Apply standardization on the VIDEO experimental stage\n",
    "    # Normalize the data from each participant w.r.t to the same variable during the corresponding resting stage.\n",
    "    df_window_norm = pd.DataFrame(data = fitted_scaler.transform(df_window), \n",
    "                                                    columns = columns_to_normalize, \n",
    "                                                    index = df_window.index)\n",
    "    \n",
    "    statistical_features = calculate_statistical_features(df_window_norm)\n",
    "\n",
    "    df_result = pd.concat([hrv_time_features, statistical_features], axis=1)\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features_from_ts_window(single_segment_ts, ORIG_SAMP_FREQUENCY_HZ,\n",
    "                                PPG_COLNAME, scaler,\n",
    "                                columns_to_normalize=TS_DATA_COLNAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying feature extraction to all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_BASED_DATASET_FILENAME = gen_path_temp(\"Dataset_AVDOSVR_ManualFeaturesHRVandStatistics\", extension=\".csv\")\n",
    "\n",
    "df_feature_extraction = None\n",
    "\n",
    "if (os.path.isfile(FEATURE_BASED_DATASET_FILENAME)):\n",
    "    df_feature_extraction = pd.read_csv(FEATURE_BASED_DATASET_FILENAME)\n",
    "    print(f\"File loaded from path!\")\n",
    "else:\n",
    "    print(f\"Generating file!\")\n",
    "\n",
    "    FIT_STATE_THRESHOLD = 7\n",
    "\n",
    "    # Iterate over participants\n",
    "    for participant in participants_ids:\n",
    "        # Iterate over segments\n",
    "        for segment in [ str(x) for x in avdosvr.preprocessing.utils.enums.AllSegments]:\n",
    "            print(f\"\\nparticipant:{participant} - segment:{segment}\")\n",
    "\n",
    "            # Standardizer per column over data in the resting stage\n",
    "            scaler = ColumnTransformer( [ \n",
    "                                        (\"\", StandardScaler(), TS_DATA_COLNAMES) # Apply to all columns with TS data\n",
    "                                    ])\n",
    "            scaler.fit(data.loc[participant])\n",
    "\n",
    "            \"\"\" Load data from VIDEO stage and filter it  \"\"\"\n",
    "            # Extract data from VIDEO stage\n",
    "            single_segment_ts = data.loc[(participant,segment)]\n",
    "            # Filter data with FitState >= average (8)\n",
    "            single_segment_ts = single_segment_ts[ single_segment_ts[\"Faceplate/FitState\"] > FIT_STATE_THRESHOLD]\n",
    "            # Choose columns with relevant time series\n",
    "            single_segment_ts = single_segment_ts[TS_DATA_COLNAMES]\n",
    "            \n",
    "            # # Filter datapoints that are larger than N*std\n",
    "            single_segment_ts = filter_outliers_from_df(single_segment_ts, 2)\n",
    "\n",
    "            \"\"\" Generate indices for sliding windows \"\"\"\n",
    "            window_indices = generate_sliding_windows(single_segment_ts,\n",
    "                                                window_width_seconds = WINDOW_WIDTH_SECS,\n",
    "                                                window_overlap_seconds = WINDOW_OVERLAP_SECS,\n",
    "                                                verbose=True)\n",
    "\n",
    "            # Iterator to generate sliding windows\n",
    "            for w_counter, (w_start, w_end) in enumerate(window_indices):\n",
    "                df_window = single_segment_ts.loc[w_start:w_end]\n",
    "\n",
    "                # To avoid non-complete window sizes. \n",
    "                # Analyze the window if the number of samples is >95% of expected number of samples\n",
    "                if(df_window.shape[0] < 0.95*WINDOW_WIDTH_SECS*ORIG_SAMP_FREQUENCY_HZ):\n",
    "                    print(f\"Skipping window: Fewer samples than expected in [{w_start},{w_end}] - Samples: {df_window.shape[0]}<95%*{WINDOW_WIDTH_SECS*ORIG_SAMP_FREQUENCY_HZ}\")\n",
    "                    continue\n",
    "            \n",
    "                # DataFrame with general information and target class in \"segment\"\n",
    "                df_this_feature_extraction = pd.DataFrame({\n",
    "                                                \"participant\":  [participant],\n",
    "                                                \"segment\":      [segment],\n",
    "                                                \"i_window\":     [w_counter],\n",
    "                                                \"w_center\":     [(w_end+w_start)/2],\n",
    "                                            })\n",
    "                \n",
    "                \"\"\" Extract HRV and statistical features from window \"\"\"\n",
    "                all_features = extract_features_from_ts_window(df_window, \n",
    "                                                                ORIG_SAMP_FREQUENCY_HZ,\n",
    "                                                                PPG_COLNAME, scaler,\n",
    "                                                                columns_to_normalize=TS_DATA_COLNAMES)\n",
    "\n",
    "                \"\"\" Create final feature vector for the window\"\"\"\n",
    "                df_this_feature_extraction = pd.concat([df_this_feature_extraction, all_features], axis=1)\n",
    "                \n",
    "                df_feature_extraction = df_this_feature_extraction if (df_feature_extraction is None) else pd.concat([df_feature_extraction, df_this_feature_extraction], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "                # Saving .csv\n",
    "                df_feature_extraction.to_csv( FEATURE_BASED_DATASET_FILENAME, index=False)\n",
    "\n",
    "                \"\"\" Save plots with HRV features \"\"\"\n",
    "                        \n",
    "                # # Save HRV features plot\n",
    "                # save_path_plot = gen_path_plot(f\"Features/Participant{participant}/HRV_{segment}_w{w}\")\n",
    "                # fig = plt.gcf().set_size_inches(8, 5)\n",
    "                # plt.tight_layout()\n",
    "                # plt.savefig(save_path_plot)\n",
    "                # plt.close(fig)\n",
    "\n",
    "                # # Save PPG plot\n",
    "                # save_path_plot = gen_path_plot(f\"Features/Participant{participant}/PPG_{segment}_w{w}\")\n",
    "                # nk.ppg_plot(signals, sampling_rate=ORIG_SAMP_FREQUENCY_HZ)\n",
    "                # fig = plt.gcf().set_size_inches(8, 5)\n",
    "                # plt.tight_layout()\n",
    "                # plt.savefig(save_path_plot)\n",
    "                # plt.close(fig)\n",
    "\n",
    "                # # # Save statistical features plot - TAKES TOO LONG!\n",
    "                # # save_path_plot = gen_path_plot(f\"Features/Participant{participant}/{segment}_{w}_ALL\")\n",
    "                # # statistical_features.plot(subplots=True,figsize=(10,2*statistical_features.shape[1]))\n",
    "                # # plt.savefig(save_path_plot)\n",
    "                # # plt.close(fig)\n",
    "\n",
    "        # df_feature_extraction.reset_index(drop=True, inplace=True)\n",
    "    print(\"\\n\\n End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save a LARGE figure with all the features\n",
    "# df_feature_extraction.plot(subplots=True,figsize=(3*participants_ids.size,2*df_feature_extraction.shape[1]))\n",
    "# save_path_plot = gen_path_plot(f\"features/_ALL\")\n",
    "# plt.savefig(save_path_plot)\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop data with unsufficient HR quality. This removes participant 38 all together.\n",
    "df_feature_extraction = df_feature_extraction[df_feature_extraction['Mean_BPM']!=-1].reset_index(drop=True)\n",
    "df_feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show number of windows for each segment to identify participants for discarding\n",
    "data_participant = df_feature_extraction.loc[(df_feature_extraction['segment'] == 'Positive')]\n",
    "data_participant = data_participant.reset_index(drop=True)\n",
    "data_participant = data_participant.participant\n",
    "data_participant.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_participant = df_feature_extraction.loc[(df_feature_extraction['segment'] == 'Negative')]\n",
    "data_participant = data_participant.reset_index(drop=True)\n",
    "data_participant = data_participant.participant\n",
    "data_participant.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_participant = df_feature_extraction.loc[(df_feature_extraction['segment'] == 'Neutral')]\n",
    "data_participant = data_participant.reset_index(drop=True)\n",
    "data_participant = data_participant.participant\n",
    "data_participant.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop participants who have less than 10 windows in any segments (see plots above).\n",
    "#df_feature_extraction.drop(df_feature_extraction.loc[df_feature_extraction['participant']==8].index, inplace=True)\n",
    "#participants_to_discard = [8, 22, 23, 25, 32, 33, 36, 14]\n",
    "participants_to_discard = [8, 22, 23, 25, 32, 33, 36, 14, 24, 28, 26, 31, 29]\n",
    "df_feature_extraction.drop(df_feature_extraction.loc[df_feature_extraction['participant'].isin(participants_to_discard)].index, inplace=True)\n",
    "df_feature_extraction = df_feature_extraction.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.A) Barplots for features vs. affect segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_extraction.loc[:,[str(x)+\"_energy\" for x in emg_colnames]].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names from feature extraction\n",
    "HRV_SUBSET_FEATURES = [\"HRV_MeanNN\",\"HRV_SDNN\",\"HRV_RMSSD\",\"HRV_MedianNN\", \"HRV_IQRNN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the feature values along the feature extraction windows\n",
    "df_features_grouped_segment = df_feature_extraction.groupby([\"participant\",\"segment\"]).mean().reset_index().drop([\"i_window\"], axis=1)\n",
    "df_features_grouped_segment = df_features_grouped_segment.set_index([\"segment\"])\n",
    "df_features_grouped_segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to generate barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_vertical_df(df_features):\n",
    "    \"\"\"\n",
    "    Changes a dataframe to vertical version to make it compatible with\n",
    "    seaborn plots.\n",
    "    \"\"\"\n",
    "    # Change columns as rows to facilitate boxplots in seaborn\n",
    "    df_vertical = None\n",
    "    for c in df_features.columns:\n",
    "        h_vertical_this = pd.DataFrame(data={\n",
    "                                            \"Affect Segment\":df_features.index.values,\n",
    "                                            \"Feature Name\":c,\n",
    "                                            \"Relative Variation\":df_features[c].values\n",
    "                                            })\n",
    "        df_vertical = h_vertical_this if (df_vertical is None) else pd.concat([df_vertical, h_vertical_this], axis=0)\n",
    "    return df_vertical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_df_features_barplot(df_subset_features, ax=None, **kwargs):\n",
    "    df_vertical = convert_to_vertical_df(df_subset_features)\n",
    "\n",
    "    PALETTE_COLORS = [avdosvr.preprocessing.utils.enums.colourPaletteRGB.Negative.value,\n",
    "                      avdosvr.preprocessing.utils.enums.colourPaletteRGB.Neutral.value,\n",
    "                      avdosvr.preprocessing.utils.enums.colourPaletteRGB.Positive.value,\n",
    "                      avdosvr.preprocessing.utils.enums.colourPaletteRGB.Relax.value]\n",
    "\n",
    "    ax_plot = sns.barplot(data=df_vertical, ax=ax,\n",
    "            x=\"Feature Name\", y=\"Relative Variation\", hue=\"Affect Segment\",\n",
    "            palette=PALETTE_COLORS,  **kwargs)\n",
    "    return ax_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heart rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_feature_colnames = [ str(x)+\"_mean\" for x in avdosvr.preprocessing.COLNAMES_HR]\n",
    "hr_feature_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_features = df_features_grouped_segment.loc[:, hr_feature_colnames]\n",
    "df_subset_features = df_subset_features.drop(['Resting_Negative'])\n",
    "df_subset_features = df_subset_features.drop(['Resting_Positive'])\n",
    "df_subset_features = df_subset_features.drop(['Resting_Neutral'])\n",
    "df_subset_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_colnames = [ x.split(\"/\")[0] for x in df_subset_features.columns]\n",
    "new_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_features.columns = new_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate path to save and figure\n",
    "save_path_plot = gen_path_plot(f\"desc-barplot-hr\")\n",
    "\n",
    "NUM_ROWS = 1\n",
    "NUM_COLS = 1\n",
    "fig,axes = plt.subplots(NUM_ROWS, NUM_COLS, sharex=False, sharey=True, figsize=(4*NUM_COLS, 5*NUM_ROWS))\n",
    "\n",
    "ax = axes\n",
    "ax_i = plot_df_features_barplot(df_subset_features, ax=ax)\n",
    "# ax_i.get_legend().remove()\n",
    "ax.grid(True)\n",
    "ax.set(xlabel=None)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heart Rate BPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for Heart Rate (BPM)\n",
    "df_subset_features = df_features_grouped_segment.loc[:, 'Mean_BPM']\n",
    "\n",
    "segments = ['Neutral', 'Positive', 'Negative']\n",
    "relax_means = [df_subset_features['Resting_Neutral'].mean(), \n",
    "               df_subset_features['Resting_Positive'].mean(), \n",
    "               df_subset_features['Resting_Negative'].mean()]\n",
    "\n",
    "segment_means = [df_subset_features['Neutral'].mean(),\n",
    "                 df_subset_features['Positive'].mean(),\n",
    "                 df_subset_features['Negative'].mean()]\n",
    "\n",
    "df_features_grouped_segment_se = df_feature_extraction.groupby([\"participant\",\"segment\"]).sem().reset_index().drop([\"i_window\"], axis=1)\n",
    "df_features_grouped_segment_se = df_features_grouped_segment_se.set_index([\"segment\"])\n",
    "bpm_se = df_features_grouped_segment_se.loc[:, 'Mean_BPM']\n",
    "\n",
    "relax_errors = [bpm_se['Resting_Neutral'].mean(), \n",
    "               bpm_se['Resting_Positive'].mean(), \n",
    "               bpm_se['Resting_Negative'].mean()]\n",
    "\n",
    "segment_errors = [bpm_se['Neutral'].mean(),\n",
    "                 bpm_se['Positive'].mean(),\n",
    "                 bpm_se['Negative'].mean()]\n",
    "\n",
    "x = np.arange(len(segments))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, relax_means, yerr=relax_errors, width=width, label='Relaxation')\n",
    "rects2 = ax.bar(x + width/2, segment_means, yerr=segment_errors, width=width, label='Affective')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Beats Per Minute (BPM)')\n",
    "ax.set_title('Mean Heart Rate per Segment')\n",
    "ax.set_xticks(x, segments)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.ylim(60,80)\n",
    "save_path_plot = gen_path_plot(f\"desc-barplot-hr-bpm\")\n",
    "plt.savefig(save_path_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HR BPM Violin Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_features = df_features_grouped_segment.loc[:, 'Mean_BPM']\n",
    "df_subset_features = df_subset_features.drop(['Resting_Negative'])\n",
    "df_subset_features = df_subset_features.drop(['Resting_Positive'])\n",
    "df_subset_features = df_subset_features.drop(['Resting_Neutral'])\n",
    "df_hr = pd.DataFrame(df_subset_features)\n",
    "\n",
    "violin_plot_colours = [avdosvr.preprocessing.utils.enums.colourPaletteRGB.Neutral.value,\n",
    "                  avdosvr.preprocessing.utils.enums.colourPaletteRGB.Positive.value,\n",
    "                  avdosvr.preprocessing.utils.enums.colourPaletteRGB.Negative.value]\n",
    "\n",
    "violin_plot_order_rule = {\n",
    "    \"Neutral\": 1,\n",
    "    \"Positive\": 2,\n",
    "    \"Negative\": 3\n",
    "    }\n",
    "\n",
    "def sort_violin_plot_order(series):\n",
    "    \"\"\"\n",
    "    Must return one Series\n",
    "    \"\"\"\n",
    "    return series.apply(lambda x: violin_plot_order_rule.get(x, 1000))\n",
    "\n",
    "df_hr = df_hr.sort_values(by=['segment'], key=sort_violin_plot_order)\n",
    "\n",
    "fig,axes = plt.subplots(1, 1, figsize=(6,4))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.violinplot(ax = axes, data=df_hr, x=df_hr.index, y=\"Mean_BPM\", palette=violin_plot_colours).set(title='Mean Heart Rate per Segment')\n",
    "\n",
    "plt.margins(x=0.05)\n",
    "axes.set(xlabel=None)\n",
    "plt.ylabel('Beats per Minute (BPM)')\n",
    "fig.tight_layout()\n",
    "save_path_plot = gen_path_plot(f\"desc-violinplot-hr-bpm\")\n",
    "plt.savefig(save_path_plot)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_bpm = df_subset_features['Neutral']\n",
    "positive_bpm = df_subset_features['Positive']\n",
    "negative_bpm = df_subset_features['Negative']\n",
    "\n",
    "f_oneway(neutral_bpm, positive_bpm, negative_bpm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HRV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HRV_SUBSET_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_features = df_features_grouped_segment.loc[:, HRV_SUBSET_FEATURES]\n",
    "df_subset_features = df_subset_features.drop([\"HRV_MeanNN\",\"HRV_MedianNN\"],axis=1)\n",
    "df_subset_features = df_subset_features.drop(['Resting_Negative'])\n",
    "df_subset_features = df_subset_features.drop(['Resting_Positive'])\n",
    "df_subset_features = df_subset_features.drop(['Resting_Neutral'])\n",
    "df_subset_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_colnames = [ x.split(\"_\")[1] for x in df_subset_features.columns]\n",
    "new_colnames = ['sdnn', 'rmssd', 'sdsd']\n",
    "new_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_features.columns = new_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate path to save and figure\n",
    "save_path_plot = gen_path_plot(f\"desc-barplot-hrv\")\n",
    "\n",
    "NUM_ROWS = 1\n",
    "NUM_COLS = 1\n",
    "fig,axes = plt.subplots(NUM_ROWS, NUM_COLS, sharex=False, sharey=True, figsize=(8*NUM_COLS, 5*NUM_ROWS))\n",
    "\n",
    "ax = axes\n",
    "ax_i = plot_df_features_barplot(df_subset_features, ax=ax, errorbar='se')\n",
    "# ax_i.get_legend().remove()\n",
    "ax.grid(True)\n",
    "ax.set(xlabel=None)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HRV Violin Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_plot = gen_path_plot(f\"desc-violinplot-hrv\")\n",
    "df_hrv = pd.DataFrame(df_subset_features)\n",
    "\n",
    "violin_plot_colours = [avdosvr.preprocessing.utils.enums.colourPaletteRGB.Neutral.value,\n",
    "                  avdosvr.preprocessing.utils.enums.colourPaletteRGB.Positive.value,\n",
    "                  avdosvr.preprocessing.utils.enums.colourPaletteRGB.Negative.value]\n",
    "\n",
    "violin_plot_order_rule = {\n",
    "    \"Neutral\": 1,\n",
    "    \"Positive\": 2,\n",
    "    \"Negative\": 3\n",
    "    }\n",
    "\n",
    "def sort_violin_plot_order(series):\n",
    "    \"\"\"\n",
    "    Must return one Series\n",
    "    \"\"\"\n",
    "    return series.apply(lambda x: violin_plot_order_rule.get(x, 1000))\n",
    "\n",
    "df_hrv = df_hrv.sort_values(by=['segment'], key=sort_violin_plot_order)\n",
    "\n",
    "hrv_sdnn = pd.DataFrame(df_hrv['sdnn']).reset_index(level=0)\n",
    "hrv_sdnn = hrv_sdnn.rename(columns={\"sdnn\": \"HRV\"})\n",
    "hrv_sdnn['type']='SDNN'\n",
    "\n",
    "hrv_rmssd = pd.DataFrame(df_hrv['rmssd']).reset_index(level=0)\n",
    "hrv_rmssd = hrv_rmssd.rename(columns={\"rmssd\": \"HRV\"})\n",
    "hrv_rmssd['type']='RMSSD'\n",
    "\n",
    "hrv_sdsd = pd.DataFrame(df_hrv['sdsd']).reset_index(level=0)\n",
    "hrv_sdsd = hrv_sdsd.rename(columns={\"sdsd\": \"HRV\"})\n",
    "hrv_sdsd['type']='SDSD'\n",
    "\n",
    "#Leave out SDSD as it's less common\n",
    "#hrv_combined = pd.concat([hrv_sdnn,hrv_rmssd,hrv_sdsd],ignore_index=True)\n",
    "hrv_combined = pd.concat([hrv_sdnn,hrv_rmssd],ignore_index=True)\n",
    "\n",
    "hrv_combined = hrv_combined.sort_values(by=['segment'], key=sort_violin_plot_order)\n",
    "\n",
    "fig,axes = plt.subplots(1, 1, figsize=(9,6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.violinplot(ax = axes, data=hrv_combined, x=\"type\", y=\"HRV\", width=0.8, palette=violin_plot_colours, hue='segment').set(title='Mean HRV per Segment')\n",
    "plt.margins(x=0.05)\n",
    "axes.set(xlabel=None)\n",
    "plt.ylabel('milliseconds')\n",
    "plt.legend(title='Affect Type',loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdnn_neutral = df_subset_features.loc['Neutral']['sdnn']\n",
    "sdnn_positive = df_subset_features.loc['Positive']['sdnn']\n",
    "sdnn_negative = df_subset_features.loc['Negative']['sdnn']\n",
    "\n",
    "rmssd_neutral = df_subset_features.loc['Neutral']['rmssd']\n",
    "rmssd_positive = df_subset_features.loc['Positive']['rmssd']\n",
    "rmssd_negative = df_subset_features.loc['Negative']['rmssd']\n",
    "\n",
    "print(f_oneway(sdnn_neutral, sdnn_positive, sdnn_negative))\n",
    "print(f_oneway(rmssd_neutral, rmssd_positive, rmssd_negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imu_feature_colnames = [ str(x)+\"_rms\" for x in imu_colnames]\n",
    "imu_feature_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_features = df_features_grouped_segment.loc[:, imu_feature_colnames]\n",
    "# df_subset_features = df_subset_features.drop([\"HRV_MeanNN\",\"HRV_MedianNN\"],axis=1)\n",
    "df_subset_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_colnames = [ str(x)[:3]+\"/\"+x.split(\".\")[1][:1] for x in df_subset_features.columns]\n",
    "df_subset_features.columns = new_colnames\n",
    "df_subset_features = df_subset_features.drop(['Resting_Negative'])\n",
    "df_subset_features = df_subset_features.drop(['Resting_Positive'])\n",
    "df_subset_features = df_subset_features.drop(['Resting_Neutral'])\n",
    "df_subset_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate path to save and figure\n",
    "save_path_plot = gen_path_plot(f\"desc-barplot-imu\")\n",
    "\n",
    "NUM_ROWS = 1\n",
    "NUM_COLS = 1\n",
    "fig,axes = plt.subplots(NUM_ROWS, NUM_COLS, sharex=False, sharey=True, figsize=(12*NUM_COLS, 5*NUM_ROWS))\n",
    "\n",
    "ax = axes\n",
    "ax_i = plot_df_features_barplot(df_subset_features, ax=ax, errorbar='se')\n",
    "# ax_i.get_legend().remove()\n",
    "ax.grid(True)\n",
    "ax.set(xlabel=None)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANOVA TESTS\n",
    "acc_x_neutral = df_subset_features.loc['Neutral']['Acc/x']\n",
    "acc_x_positive = df_subset_features.loc['Positive']['Acc/x']\n",
    "acc_x_negative = df_subset_features.loc['Negative']['Acc/x']\n",
    "\n",
    "acc_y_neutral = df_subset_features.loc['Neutral']['Acc/y']\n",
    "acc_y_positive = df_subset_features.loc['Positive']['Acc/y']\n",
    "acc_y_negative = df_subset_features.loc['Negative']['Acc/y']\n",
    "\n",
    "acc_z_neutral = df_subset_features.loc['Neutral']['Acc/z']\n",
    "acc_z_positive = df_subset_features.loc['Positive']['Acc/z']\n",
    "acc_z_negative = df_subset_features.loc['Negative']['Acc/z']\n",
    "\n",
    "mag_x_neutral = df_subset_features.loc['Neutral']['Mag/x']\n",
    "mag_x_positive = df_subset_features.loc['Positive']['Mag/x']\n",
    "mag_x_negative = df_subset_features.loc['Negative']['Mag/x']\n",
    "\n",
    "mag_y_neutral = df_subset_features.loc['Neutral']['Mag/y']\n",
    "mag_y_positive = df_subset_features.loc['Positive']['Mag/y']\n",
    "mag_y_negative = df_subset_features.loc['Negative']['Mag/y']\n",
    "\n",
    "mag_z_neutral = df_subset_features.loc['Neutral']['Mag/z']\n",
    "mag_z_positive = df_subset_features.loc['Positive']['Mag/z']\n",
    "mag_z_negative = df_subset_features.loc['Negative']['Mag/z']\n",
    "\n",
    "gyr_x_neutral = df_subset_features.loc['Neutral']['Gyr/x']\n",
    "gyr_x_positive = df_subset_features.loc['Positive']['Gyr/x']\n",
    "gyr_x_negative = df_subset_features.loc['Negative']['Gyr/x']\n",
    "\n",
    "gyr_y_neutral = df_subset_features.loc['Neutral']['Gyr/y']\n",
    "gyr_y_positive = df_subset_features.loc['Positive']['Gyr/y']\n",
    "gyr_y_negative = df_subset_features.loc['Negative']['Gyr/y']\n",
    "\n",
    "gyr_z_neutral = df_subset_features.loc['Neutral']['Gyr/z']\n",
    "gyr_z_positive = df_subset_features.loc['Positive']['Gyr/z']\n",
    "gyr_z_negative = df_subset_features.loc['Negative']['Gyr/z']\n",
    "\n",
    "print('Acc_X: ' + str(f_oneway(acc_x_neutral, acc_x_positive, acc_x_negative)))\n",
    "print('Acc_Y: ' + str(f_oneway(acc_y_neutral, acc_y_positive, acc_y_negative)))\n",
    "print('Acc_Z: ' + str(f_oneway(acc_z_neutral, acc_z_positive, acc_z_negative)))\n",
    "print('Mag_X: ' + str(f_oneway(mag_x_neutral, mag_x_positive, mag_x_negative)))\n",
    "print('Mag_Y: ' + str(f_oneway(mag_y_neutral, mag_y_positive, mag_y_negative)))\n",
    "print('Mag_Z ' + str(f_oneway(mag_z_neutral, mag_z_positive, mag_z_negative)))\n",
    "print('Gyr_X: ' + str(f_oneway(gyr_x_neutral, gyr_x_positive, gyr_x_negative)))\n",
    "print('Gyr_Y: ' + str(f_oneway(gyr_y_neutral, gyr_y_positive, gyr_y_negative)))\n",
    "print('Gyr_Z: ' + str(f_oneway(gyr_z_neutral, gyr_z_positive, gyr_z_negative)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMG Amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_feature_colnames = [ str(x)+\"_rms\" for x in emg_colnames]\n",
    "emg_feature_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_features = df_features_grouped_segment.loc[:, emg_feature_colnames]\n",
    "df_subset_features = df_subset_features.drop(['Resting_Negative'])\n",
    "df_subset_features = df_subset_features.drop(['Resting_Positive'])\n",
    "df_subset_features = df_subset_features.drop(['Resting_Neutral'])\n",
    "df_subset_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_colnames = [ x.split(\"[\")[1].split(\"]\")[0] for x in df_subset_features.columns]\n",
    "new_colnames\n",
    "df_subset_features.columns = new_colnames\n",
    "rearranged_emg_colnames = ['LeftFrontalis', 'RightFrontalis', 'CenterCorrugator', 'LeftOrbicularis', \n",
    "                           'RightOrbicularis', 'LeftZygomaticus', 'RightZygomaticus']\n",
    "df_subset_features = df_subset_features[rearranged_emg_colnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate path to save and figure\n",
    "save_path_plot = gen_path_plot(f\"desc-barplot-emgAmplitude\")\n",
    "\n",
    "NUM_ROWS = 1\n",
    "NUM_COLS = 1\n",
    "fig,axes = plt.subplots(NUM_ROWS, NUM_COLS, sharex=False, sharey=True, figsize=(10*NUM_COLS, 5*NUM_ROWS))\n",
    "\n",
    "PALETTE_COLORS = [avdosvr.preprocessing.utils.enums.colourPaletteRGB.Negative.value,\n",
    "                  avdosvr.preprocessing.utils.enums.colourPaletteRGB.Neutral.value,\n",
    "                  avdosvr.preprocessing.utils.enums.colourPaletteRGB.Positive.value,\n",
    "                  avdosvr.preprocessing.utils.enums.colourPaletteRGB.Relax.value]\n",
    "\n",
    "df_vertical = convert_to_vertical_df(df_subset_features)\n",
    "\n",
    "ax = axes\n",
    "ax_i = ax_plot = sns.barplot(data=df_vertical, ax=ax, x=\"Feature Name\", y=\"Relative Variation\", hue=\"Affect Segment\", \n",
    "                      palette=PALETTE_COLORS, errorbar='se').set(title='EMG Amplitude per each segment/muscle group')\n",
    "\n",
    "# Signififance annotations\n",
    "\n",
    "## Positive - Neutral\n",
    "### CCorrugator\n",
    "plt.plot([2,2, 2.25, 2.25], [0.94, 0.96, 0.96, 0.94], linewidth=1, color='k')\n",
    "plt.text((2+2.25)*.5, 0.95, \"**\", ha='center', va='bottom', color='k')\n",
    "\n",
    "### LOrbicularis\n",
    "plt.plot([3,3, 3.25, 3.25], [0.85, 0.87, 0.87, 0.85], linewidth=1, color='k')\n",
    "plt.text((3+3.25)*.5, 0.86, \"***\", ha='center', va='bottom', color='k')\n",
    "\n",
    "### ROrbicularis\n",
    "plt.plot([4,4, 4.27, 4.27], [0.88, 0.90, 0.90, 0.88], linewidth=1, color='k')\n",
    "plt.text((4+4.27)*.5, 0.89, \"***\", ha='center', va='bottom', color='k')\n",
    "\n",
    "### Lzygomaticus\n",
    "plt.plot([5,5, 5.26, 5.26], [0.96, 0.98, 0.98, 0.96], linewidth=1, color='k')\n",
    "plt.text((5+5.26)*.5, 0.97, \"***\", ha='center', va='bottom', color='k')\n",
    "\n",
    "### Rzygomaticus\n",
    "plt.plot([6,6, 6.27, 6.27], [0.96, 0.98, 0.98, 0.96], linewidth=1, color='k')\n",
    "plt.text((6+6.27)*.5, 0.97, \"***\", ha='center', va='bottom', color='k')\n",
    "\n",
    "\n",
    "\n",
    "## Positive - Negative\n",
    "### CCorrugator\n",
    "plt.plot([1.74,1.74, 2.25, 2.25], [0.9, 0.92, 0.92, 0.9], linewidth=1, color='k')\n",
    "plt.text((1.6+2.25)*.5, 0.91, \"**\", ha='center', va='bottom', color='k')\n",
    "\n",
    "### LOrbicularis\n",
    "plt.plot([2.74,2.74, 3.25, 3.25], [0.89, 0.91, 0.91, 0.89], linewidth=1, color='k')\n",
    "plt.text((2.74+3.25)*.5, 0.90, \"***\", ha='center', va='bottom', color='k')\n",
    "\n",
    "### ROrbicularis\n",
    "plt.plot([3.74,3.74, 4.27, 4.27], [0.92, 0.94, 0.94, 0.92], linewidth=1, color='k')\n",
    "plt.text((3.74+4.27)*.5, 0.93, \"***\", ha='center', va='bottom', color='k')\n",
    "\n",
    "### Lzygomaticus\n",
    "plt.plot([4.74,4.74, 5.26, 5.26], [1, 1.02, 1.02, 1], linewidth=1, color='k')\n",
    "plt.text((4.74+5.26)*.5, 1.01, \"***\", ha='center', va='bottom', color='k')\n",
    "\n",
    "### Rzygomaticus\n",
    "plt.plot([5.74,5.74, 6.27, 6.27], [1, 1.02, 1.02, 1], linewidth=1, color='k')\n",
    "plt.text((5.74+6.27)*.5, 1.01, \"***\", ha='center', va='bottom', color='k')\n",
    "\n",
    "\n",
    "\n",
    "## Positive - Negative\n",
    "### Rfrontalis\n",
    "plt.plot([0.73,0.73, 1, 1], [0.74, 0.76, 0.76, 0.74], linewidth=1, color='k')\n",
    "plt.text((0.73+1)*.5, 0.75, \"**\", ha='center', va='bottom', color='k')\n",
    "\n",
    "### CCorrugator\n",
    "plt.plot([1.74,1.74, 2, 2], [0.98, 1, 1, 0.98], linewidth=1, color='k')\n",
    "plt.text((1.74+2)*.5, 0.99, \"***\", ha='center', va='bottom', color='k')\n",
    "\n",
    "\n",
    "ax.grid(True)\n",
    "ax.set(xlabel=None)\n",
    "plt.ylabel('EMG Amplitude (standarised)')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANOVA TESTS\n",
    "left_frontalis_neutral = df_subset_features.loc['Neutral']['LeftFrontalis']\n",
    "left_frontalis_positive = df_subset_features.loc['Positive']['LeftFrontalis']\n",
    "left_frontalis_negative = df_subset_features.loc['Negative']['LeftFrontalis']\n",
    "\n",
    "right_frontalis_neutral = df_subset_features.loc['Neutral']['RightFrontalis']\n",
    "right_frontalis_positive = df_subset_features.loc['Positive']['RightFrontalis']\n",
    "right_frontalis_negative = df_subset_features.loc['Negative']['RightFrontalis']\n",
    "\n",
    "center_corrugator_neutral = df_subset_features.loc['Neutral']['CenterCorrugator']\n",
    "center_corrugator_positive = df_subset_features.loc['Positive']['CenterCorrugator']\n",
    "center_corrugator_negative = df_subset_features.loc['Negative']['CenterCorrugator']\n",
    "\n",
    "left_orbicularis_neutral = df_subset_features.loc['Neutral']['LeftOrbicularis']\n",
    "left_orbicularis_positive = df_subset_features.loc['Positive']['LeftOrbicularis']\n",
    "left_orbicularis_negative = df_subset_features.loc['Negative']['LeftOrbicularis']\n",
    "\n",
    "right_orbicularis_neutral = df_subset_features.loc['Neutral']['RightOrbicularis']\n",
    "right_orbicularis_positive = df_subset_features.loc['Positive']['RightOrbicularis']\n",
    "right_orbicularis_negative = df_subset_features.loc['Negative']['RightOrbicularis']\n",
    "\n",
    "left_zygomaticus_neutral = df_subset_features.loc['Neutral']['LeftZygomaticus']\n",
    "left_zygomaticus_positive = df_subset_features.loc['Positive']['LeftZygomaticus']\n",
    "left_zygomaticus_negative = df_subset_features.loc['Negative']['LeftZygomaticus']\n",
    "\n",
    "right_zygomaticus_neutral = df_subset_features.loc['Neutral']['RightZygomaticus']\n",
    "right_zygomaticus_positive = df_subset_features.loc['Positive']['RightZygomaticus']\n",
    "right_zygomaticus_negative = df_subset_features.loc['Negative']['RightZygomaticus']\n",
    "\n",
    "print('Lfrontalis: ' + str(f_oneway(left_frontalis_neutral, left_frontalis_positive, left_frontalis_negative)))\n",
    "print('Rfrontalis: ' + str(f_oneway(right_frontalis_neutral, right_frontalis_positive, right_frontalis_negative)))\n",
    "print('Ccorrugator: ' + str(f_oneway(center_corrugator_neutral, center_corrugator_positive, center_corrugator_negative)))\n",
    "print('LOrbicularis: ' + str(f_oneway(left_orbicularis_neutral, left_orbicularis_positive, left_orbicularis_negative)))\n",
    "print('ROrbicularis: ' + str(f_oneway(right_orbicularis_neutral, right_orbicularis_positive, right_orbicularis_negative)))\n",
    "print('Lzygomaticus: ' + str(f_oneway(left_zygomaticus_neutral, left_zygomaticus_positive, left_zygomaticus_negative)))\n",
    "print('Rzygomaticus: ' + str(f_oneway(right_zygomaticus_neutral, right_zygomaticus_positive, right_zygomaticus_negative)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T-TESTS\n",
    "\n",
    "bonferroni_correction = 7\n",
    "decimal_places = 3\n",
    "print('Bonferonni correction for 7 tests applied. P value multiplied by 7.\\n')\n",
    "#Positive - Neutral\n",
    "Pos_Neu_LF_test_value, Pos_Neu_LF_p_value, = stats.ttest_rel(left_frontalis_positive, left_frontalis_neutral)\n",
    "Pos_Neu_RF_test_value, Pos_Neu_RF_p_value, = stats.ttest_rel(right_frontalis_positive, right_frontalis_neutral)\n",
    "Pos_Neu_CC_test_value, Pos_Neu_CC_p_value, = stats.ttest_rel(center_corrugator_positive, center_corrugator_neutral)\n",
    "Pos_Neu_LO_test_value, Pos_Neu_LO_p_value, = stats.ttest_rel(left_orbicularis_positive, left_orbicularis_neutral)\n",
    "Pos_Neu_RO_test_value, Pos_Neu_RO_p_value, = stats.ttest_rel(right_orbicularis_positive, right_orbicularis_neutral)\n",
    "Pos_Neu_LZ_test_value, Pos_Neu_LZ_p_value, = stats.ttest_rel(left_zygomaticus_positive, left_zygomaticus_neutral)\n",
    "Pos_Neu_RZ_test_value, Pos_Neu_RZ_p_value, = stats.ttest_rel(right_zygomaticus_positive, right_zygomaticus_neutral)\n",
    "\n",
    "print('Positive - Neutral')\n",
    "print('Lfrontalis - statistic=' + str(round(Pos_Neu_LF_test_value, decimal_places)) + ', p=' + str(round(Pos_Neu_LF_p_value*bonferroni_correction, decimal_places)))\n",
    "print('Rfrontalis - statistic=' + str(round(Pos_Neu_RF_test_value, decimal_places)) + ', p=' + str(round(Pos_Neu_RF_p_value*bonferroni_correction, decimal_places)))\n",
    "print('Ccorrugator - statistic=' + str(round(Pos_Neu_CC_test_value, decimal_places)) + ', p=' + str(round(Pos_Neu_CC_p_value*bonferroni_correction, decimal_places)))\n",
    "print('LOrbicularis - statistic=' + str(round(Pos_Neu_LO_test_value, decimal_places)) + ', p=' + str(round(Pos_Neu_LO_p_value*bonferroni_correction, decimal_places)))\n",
    "print('ROrbicularis - statistic=' + str(round(Pos_Neu_RO_test_value, decimal_places)) + ', p=' + str(round(Pos_Neu_RO_p_value*bonferroni_correction, decimal_places)))\n",
    "print('Lzygomaticus - statistic=' + str(round(Pos_Neu_LZ_test_value, decimal_places)) + ', p=' + str(round(Pos_Neu_LZ_p_value*bonferroni_correction, decimal_places)))\n",
    "print('Rzygomaticus - statistic=' + str(round(Pos_Neu_RZ_test_value, decimal_places)) + ', p=' + str(round(Pos_Neu_RZ_p_value*bonferroni_correction, decimal_places)))\n",
    "\n",
    "#Positive - Negative\n",
    "Pos_Neg_LF_test_value, Pos_Neg_LF_p_value, = stats.ttest_rel(left_frontalis_positive, left_frontalis_negative)\n",
    "Pos_Neg_RF_test_value, Pos_Neg_RF_p_value, = stats.ttest_rel(right_frontalis_positive, right_frontalis_negative)\n",
    "Pos_Neg_CC_test_value, Pos_Neg_CC_p_value, = stats.ttest_rel(center_corrugator_positive, center_corrugator_negative)\n",
    "Pos_Neg_LO_test_value, Pos_Neg_LO_p_value, = stats.ttest_rel(left_orbicularis_positive, left_orbicularis_negative)\n",
    "Pos_Neg_RO_test_value, Pos_Neg_RO_p_value, = stats.ttest_rel(right_orbicularis_positive, right_orbicularis_negative)\n",
    "Pos_Neg_LZ_test_value, Pos_Neg_LZ_p_value, = stats.ttest_rel(left_zygomaticus_positive, left_zygomaticus_negative)\n",
    "Pos_Neg_RZ_test_value, Pos_Neg_RZ_p_value, = stats.ttest_rel(right_zygomaticus_positive, right_zygomaticus_negative)\n",
    "\n",
    "print('\\nPositive - Negative')\n",
    "print('Lfrontalis - statistic=' + str(round(Pos_Neg_LF_test_value, decimal_places)) + ', p=' + str(round(Pos_Neg_LF_p_value*bonferroni_correction, decimal_places)))\n",
    "print('Rfrontalis - statistic=' + str(round(Pos_Neg_RF_test_value, decimal_places)) + ', p=' + str(round(Pos_Neg_RF_p_value*bonferroni_correction, decimal_places)))\n",
    "print('Ccorrugator - statistic=' + str(round(Pos_Neg_CC_test_value, decimal_places)) + ', p=' + str(round(Pos_Neg_CC_p_value*bonferroni_correction, decimal_places)))\n",
    "print('LOrbicularis - statistic=' + str(round(Pos_Neg_LO_test_value, decimal_places)) + ', p=' + str(round(Pos_Neg_LO_p_value*bonferroni_correction, decimal_places)))\n",
    "print('ROrbicularis - statistic=' + str(round(Pos_Neg_RO_test_value, decimal_places)) + ', p=' + str(round(Pos_Neg_RO_p_value*bonferroni_correction, decimal_places)))\n",
    "print('Lzygomaticus - statistic=' + str(round(Pos_Neg_LZ_test_value, decimal_places)) + ', p=' + str(round(Pos_Neg_LZ_p_value*bonferroni_correction, decimal_places)))\n",
    "print('Rzygomaticus - statistic=' + str(round(Pos_Neg_RZ_test_value, decimal_places)) + ', p=' + str(round(Pos_Neg_RZ_p_value*bonferroni_correction, decimal_places)))\n",
    "\n",
    "#Negative - Neutral\n",
    "Neg_Neu_LF_test_value, Neg_Neu_LF_p_value, = stats.ttest_rel(left_frontalis_negative, left_frontalis_neutral)\n",
    "Neg_Neu_RF_test_value, Neg_Neu_RF_p_value, = stats.ttest_rel(right_frontalis_negative, right_frontalis_neutral)\n",
    "Neg_Neu_CC_test_value, Neg_Neu_CC_p_value, = stats.ttest_rel(center_corrugator_negative, center_corrugator_neutral)\n",
    "Neg_Neu_LO_test_value, Neg_Neu_LO_p_value, = stats.ttest_rel(left_orbicularis_negative, left_orbicularis_neutral)\n",
    "Neg_Neu_RO_test_value, Neg_Neu_RO_p_value, = stats.ttest_rel(right_orbicularis_negative, right_orbicularis_neutral)\n",
    "Neg_Neu_LZ_test_value, Neg_Neu_LZ_p_value, = stats.ttest_rel(left_zygomaticus_negative, left_zygomaticus_neutral)\n",
    "Neg_Neu_RZ_test_value, Neg_Neu_RZ_p_value, = stats.ttest_rel(right_zygomaticus_negative, right_zygomaticus_neutral)\n",
    "\n",
    "print('\\nNegative - Neutral')\n",
    "print('Lfrontalis - statistic=' + str(round(Neg_Neu_LF_test_value, decimal_places)) + ', p=' + str(round(Neg_Neu_LF_p_value*bonferroni_correction, decimal_places)))\n",
    "print('Rfrontalis - statistic=' + str(round(Neg_Neu_RF_test_value, decimal_places)) + ', p=' + str(round(Neg_Neu_RF_p_value*bonferroni_correction, decimal_places)))\n",
    "print('Ccorrugator - statistic=' + str(round(Neg_Neu_CC_test_value, decimal_places)) + ', p=' + str(round(Neg_Neu_CC_p_value*bonferroni_correction, decimal_places)))\n",
    "print('LOrbicularis - statistic=' + str(round(Neg_Neu_LO_test_value, decimal_places)) + ', p=' + str(round(Neg_Neu_LO_p_value*bonferroni_correction, decimal_places)))\n",
    "print('ROrbicularis - statistic=' + str(round(Neg_Neu_RO_test_value, decimal_places)) + ', p=' + str(round(Neg_Neu_RO_p_value*bonferroni_correction, decimal_places)))\n",
    "print('Lzygomaticus - statistic=' + str(round(Neg_Neu_LZ_test_value, decimal_places)) + ', p=' + str(round(Neg_Neu_LZ_p_value*bonferroni_correction, decimal_places)))\n",
    "print('Rzygomaticus - statistic=' + str(round(Neg_Neu_RZ_test_value, decimal_places)) + ', p=' + str(round(Neg_Neu_RZ_p_value*bonferroni_correction, decimal_places)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMG Contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_contact_feature_colnames = [ str(x)+\"_rms\" for x in emg_contact_colnames]\n",
    "emg_contact_feature_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_features = df_features_grouped_segment.loc[:, emg_contact_feature_colnames]\n",
    "df_subset_features = df_subset_features.drop(['Resting_Negative'])\n",
    "df_subset_features = df_subset_features.drop(['Resting_Positive'])\n",
    "df_subset_features = df_subset_features.drop(['Resting_Neutral'])\n",
    "df_subset_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_colnames = [ x.split(\"[\")[1].split(\"]\")[0] for x in df_subset_features.columns]\n",
    "new_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_features.columns = new_colnames\n",
    "df_subset_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate path to save and figure\n",
    "save_path_plot = gen_path_plot(f\"desc-barplot-emgContact\")\n",
    "\n",
    "NUM_ROWS = 1\n",
    "NUM_COLS = 1\n",
    "fig,axes = plt.subplots(NUM_ROWS, NUM_COLS, sharex=False, sharey=True, figsize=(10*NUM_COLS, 5*NUM_ROWS))\n",
    "\n",
    "ax = axes\n",
    "ax_i = plot_df_features_barplot(df_subset_features, ax=ax, errorbar='se')\n",
    "# ax_i.get_legend().remove()\n",
    "ax.grid(True)\n",
    "ax.set(xlabel=None)\n",
    "# ax.tick_params(axis='x', labelrotation = 45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis 3: Classification\n",
    "\n",
    "- Select subset of features: `[Cardiac, Motor, Facial]`\n",
    "- Configure the train-test strategy for CV with [Leave-One-Subject-Out (LOSO)](https://scikit-learn.org/stable/modules/cross_validation.html#leave-one-group-out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to define the segments of interest and the mapping for classifiers\n",
    "CLASSES_MAPPING = {\n",
    "            \"Negative\": -1,\n",
    "            \"Neutral\": 0,\n",
    "            \"Positive\": 1,\n",
    "            }\n",
    "CLASSES_MAPPING_INVERSE = { v:k for k,v in CLASSES_MAPPING.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = df_feature_extraction.loc[(df_feature_extraction['segment'] == 'Positive')  | \n",
    "                                   (df_feature_extraction['segment'] == 'Negative') | \n",
    "                                   (df_feature_extraction['segment'] == 'Neutral')]\n",
    "data_X = data_X.reset_index(drop=True)\n",
    "data_X = data_X.drop([\"segment\"], axis=1)\n",
    "data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Y = df_feature_extraction.loc[(df_feature_extraction['segment'] == 'Positive')  | \n",
    "                                   (df_feature_extraction['segment'] == 'Negative') | \n",
    "                                   (df_feature_extraction['segment'] == 'Neutral')]\n",
    "data_Y = data_Y.reset_index(drop=True)\n",
    "data_Y = data_Y[\"segment\"].map(CLASSES_MAPPING)\n",
    "data_Y.value_counts().plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_participant = df_feature_extraction.loc[(df_feature_extraction['segment'] == 'Positive')  | \n",
    "                                   (df_feature_extraction['segment'] == 'Negative') | \n",
    "                                   (df_feature_extraction['segment'] == 'Neutral')]\n",
    "data_participant = data_participant.reset_index(drop=True)\n",
    "data_participant = data_participant.participant\n",
    "data_participant.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection per data modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrv_colnames = data_X.columns[ [ (col.startswith(\"HRV\") | col.startswith(\"HeartRate\") | col.startswith(\"Ppg/\")) for col in data_X.columns] ].sort_values().values\n",
    "hrv_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imu_colnames = data_X.columns[ [ (col.startswith(\"Accelerometer\")) | (col.startswith(\"Magnetometer\")) | (col.startswith(\"Gyroscope\")) for col in data_X.columns] ].sort_values().values\n",
    "imu_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_amp_colnames = data_X.columns[ [ (col.startswith(\"Emg/Amplitude\")) for col in data_X.columns] ].sort_values().values\n",
    "emg_amp_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_cont_colnames = data_X.columns[ [ (col.startswith(\"Emg/Contact\")) for col in data_X.columns] ].sort_values().values\n",
    "emg_cont_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"HRV {hrv_colnames.size}\")\n",
    "print(f\"IMU {imu_colnames.size}\")\n",
    "print(f\"EMG Amplitude {emg_amp_colnames.size}\")\n",
    "print(f\"EMG Contact {emg_cont_colnames.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation plots\n",
    "\n",
    "Standardize [$\\mathcal{N}(0,1)$] each of the column features and map the correlation among variables and vs. target variable `segment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name and columns of the corresponding data modalities\n",
    "corr_plots_config = {\n",
    "    \"hrv\": hrv_colnames,\n",
    "    \"imu\": imu_colnames,\n",
    "    \"emg_amp\": emg_amp_colnames,\n",
    "    \"emg_cont\": emg_cont_colnames,\n",
    "}\n",
    "\n",
    "for k,v in corr_plots_config.items():\n",
    "    # Select subfeatures from large dataset\n",
    "    df_plot = df_feature_extraction[v]\n",
    "    # Standardize the features (but not the target)\n",
    "    df_plot = pd.DataFrame(data=StandardScaler().fit_transform(df_plot), \n",
    "                            columns=df_plot.columns, \n",
    "                            index=df_plot.index)\n",
    "    # Concatenate target and features from the specific modality\n",
    "    df_plot = pd.concat([data_Y, df_plot],axis=1)\n",
    "\n",
    "    # Generate correlation plot and save DataFrame as HTML (it's not a matplotlib Figure)\n",
    "    save_path_plot = gen_path_plot(f\"Features/_CorrelationPlot_{k}\", extension=\".html\")\n",
    "    corr = df_plot.corr()\n",
    "    corr_style = corr.style.background_gradient(cmap='coolwarm', axis=None).format(precision=2)\n",
    "    corr_style.to_html(save_path_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification models\n",
    "\n",
    "Important reading: [Common pitfalls in the interpretation of coeffs in linear models.](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#sphx-glr-auto-examples-inspection-plot-linear-model-coefficient-interpretation-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE-BASED CLASSIFIERS CLASSIFIERS SETUP\n",
    "MC_RANDOM_SEED = 1234\n",
    "N_SPLITS_CV = 10 # Number of folds for Cross-validation\n",
    "\n",
    "# Scoring parameters: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "SCORING_METRICS = [\"accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Testing Classification pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example of iterator for Cross-validation per subject\n",
    "\n",
    "# Feature subset\n",
    "feature_subset_colnames = hrv_colnames   # imu_colnames, emg_amp_colnames, emg_cont_colnames\n",
    "\n",
    "# Features\n",
    "x = data_X[feature_subset_colnames]\n",
    "# Target\n",
    "y = data_Y\n",
    "# Groups indices (participants' ids)\n",
    "group_cv = data_participant\n",
    "\n",
    "loso_cv = LeaveOneGroupOut()\n",
    "cv_splits = loso_cv.split(x, y, groups=group_cv)\n",
    "for trn_idx, tst_idx in cv_splits:\n",
    "    print(\"TRN: %s \\t TST: %s\" % (data_participant[trn_idx].unique(), data_participant[tst_idx].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modality_colnames = { \n",
    "            \"hrv\": hrv_colnames, \n",
    "            \"imu\": imu_colnames, \n",
    "            \"emg_amp\": emg_amp_colnames,\n",
    "            \"emg_cont\": emg_cont_colnames,\n",
    "            \"all\": list(hrv_colnames) + list(imu_colnames) + list(emg_amp_colnames) + list(emg_cont_colnames),\n",
    "    }\n",
    "\n",
    "# ClassifierName: {\"clf\":model, \"pgrid\":parameters)\n",
    "classifiers_hyperparams = {\n",
    "    \"LinearRidge\": {    \"clf\": RidgeClassifier(alpha=0.01, max_iter=1000), # n_class classifiers are trained in a one-versus-all approach. Concretely, taking advantage of the multi-variate response support in Ridge\n",
    "                        \"pgrid\": {'alpha': np.logspace(-5, 5,11) }},\n",
    "    \"GaussianSVM\": {    \"clf\": SVC(kernel='rbf', gamma='auto', C = 1),          # Multilabel in one-vs-one approach\n",
    "                        \"pgrid\": {'C': [1, 10, 100, 1000], 'gamma': [0.1, 0.01, 0.001]}},\n",
    "    \"RF\": {             \"clf\": RandomForestClassifier(criterion='entropy', random_state=MC_RANDOM_SEED, class_weight=\"balanced\"), # Multilabel classification\n",
    "                        \"pgrid\": {'n_estimators': [10, 50, 100], 'max_depth': [5, 10, 20]}},\n",
    "    \"KNN\": {            \"clf\": KNeighborsClassifier(),\n",
    "                        \"pgrid\": {'n_neighbors': [1, 5, 11, 15]}},\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution below takes around 150min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all segments for all participants and store the resting and video parts in a single large CSV.\n",
    "\n",
    "DATASET_POSTPROCESSED_FILENAME = gen_path_temp(\"Results_ModelTrainingCV_PerDataModalityPerSubject\", extension=\".csv\")\n",
    "\n",
    "output_filename = DATASET_POSTPROCESSED_FILENAME\n",
    "\n",
    "# Variable to store the final dataset\n",
    "df_results_hyperparam_opt = None\n",
    "\n",
    "# Check if file already exists\n",
    "if (os.path.isfile(output_filename)):\n",
    "    df_results_hyperparam_opt = pd.read_csv(output_filename)\n",
    "    print(f\"File loaded from path!\")\n",
    "# Otherwise generate it\n",
    "else:\n",
    "    print(f\"Generating file!\")\n",
    "    \n",
    "    ## Iteration per data type\n",
    "    for modality_name, modality_colnames in data_modality_colnames.items(): \n",
    "\n",
    "        # modality_name = \"hrv\"\n",
    "        # modality_colnames = data_modality_colnames[modality_name]\n",
    "        #### ABOVE FOR TESTING\n",
    "\n",
    "        # Subset of features\n",
    "        data_mod_x = data_X[modality_colnames].values.copy()   # Features\n",
    "        data_mod_y = data_Y.values.copy()       # Target\n",
    "        subject_ids = data_participant.values.copy()  # Groups indices (participants' ids)\n",
    "\n",
    "        # Split dataset with LOSO-CV\n",
    "        cv_loso_subj = LeaveOneGroupOut()\n",
    "        cv_splits_subjects = cv_loso_subj.split(data_mod_x, data_mod_y, groups=subject_ids)\n",
    "\n",
    "        # Iteration per subject (participant)\n",
    "        for trn_subj_idx, tst_subj_idx in cv_splits_subjects:\n",
    "\n",
    "            # The dataset that is not belonging to the TEST subject will be further divided for hyperparam optimization.\n",
    "            x = data_mod_x[trn_subj_idx]             # Data to be used to create a model for TEST subject\n",
    "            x_test_subj = data_mod_x[tst_subj_idx]\n",
    "            y = data_mod_y[trn_subj_idx]\n",
    "            y_test_subj = data_mod_y[tst_subj_idx]\n",
    "            subjects_cv = data_participant[trn_subj_idx].values\n",
    "            subject_in_test_set = np.unique(data_participant[tst_subj_idx].values)[0]   # Store the participant id in the test set\n",
    "            \n",
    "            # print(f\"TRAIN SUBJECT IDS: {np.unique(subjects_cv)} \\t TEST SUBJECT: {subject_in_test_set}\")\n",
    "            # print(f\"SHAPE : x:{x.shape}, x_test_subj:{x_test_subj.shape}, y:{y.shape}, y_test_subj:{y_test_subj.shape}, subjects_cv:{subjects_cv.shape}\")\n",
    "\n",
    "            # Create pipeline\n",
    "            scaler = StandardScaler().fit(x)\n",
    "            x_scaled = scaler.transform(x)\n",
    "\n",
    "            for clf_name, clf_data in classifiers_hyperparams.items(): \n",
    "                # clf_name = \"GaussianSVM\"\n",
    "                # clf_data = classifiers_hyperparams[clf_name]\n",
    "                #### ABOVE FOR TESTING\n",
    "\n",
    "                clf = clf_data[\"clf\"]\n",
    "                pgrid = clf_data[\"pgrid\"]\n",
    "                \n",
    "                # Leave-One-Subject-Out CV also to optimize the hyperparameters and select a model\n",
    "                cv_loso_fold = LeaveOneGroupOut()\n",
    "                cv_fold_per_subject = cv_loso_subj.split(x, y, groups = subjects_cv)    \n",
    "\n",
    "                gr_search = GridSearchCV(clf, pgrid, cv=cv_fold_per_subject, scoring=SCORING_METRICS, refit=\"accuracy\", n_jobs=-1)\n",
    "                gr_search.fit(x_scaled, y)\n",
    "\n",
    "                # Get results per fold and add best results\n",
    "                df_this_hyperparam_optim = pd.DataFrame(gr_search.cv_results_)\n",
    "                df_this_hyperparam_optim.insert(0,\"best_trn_score_\", str(gr_search.best_score_))\n",
    "                df_this_hyperparam_optim.insert(0,\"best_params_\", str(gr_search.best_params_))\n",
    "                df_this_hyperparam_optim.insert(0,\"best_estimator_\", str(gr_search.best_estimator_))\n",
    "\n",
    "                # Insert general information in long format\n",
    "                df_this_hyperparam_optim.insert(0,\"classifier\", clf_name)\n",
    "                df_this_hyperparam_optim.insert(0, \"test_subject_id\",subject_in_test_set)\n",
    "                df_this_hyperparam_optim.insert(0, \"data_modality\", modality_name)\n",
    "                df_this_hyperparam_optim.insert(0, \"pipeline_step\", \"hyperparam_opt\")\n",
    "\n",
    "                # Append to the main dataframe with the results \n",
    "                df_results_hyperparam_opt = df_this_hyperparam_optim if (df_results_hyperparam_opt is None) else pd.concat([df_results_hyperparam_opt, df_this_hyperparam_optim], axis=0, ignore_index=True)\n",
    "                \n",
    "                print(f\"Data modality: {modality_name} | Clf: {clf_name} | Subject: {subject_in_test_set} \")\n",
    "\n",
    "                # Saving .csv\n",
    "                df_results_hyperparam_opt.to_csv( output_filename, index=False)\n",
    "\n",
    "            # End of classifiers\n",
    "        # End of subjects\n",
    "\n",
    "        # # Saving .csv every iteration\n",
    "        # df_results_hyperparam_opt.to_csv( output_filename )\n",
    "    print(\"\\n\\n End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_hyperparam_opt.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over participants to know the best model per subject and its hyperparams.\n",
    "for participant in participants_ids:\n",
    "    # participant = 0\n",
    "    query = ((df_results_hyperparam_opt.test_subject_id == participant) & \\\n",
    "                (df_results_hyperparam_opt.rank_test_accuracy == 1) & \\\n",
    "                    (df_results_hyperparam_opt.data_modality == \"all\") )\n",
    "    best_results_participant = df_results_hyperparam_opt[ query ]\n",
    "    best_classifier_gridsearch = best_results_participant[ best_results_participant.mean_test_accuracy == best_results_participant.mean_test_accuracy.max() ]\n",
    "    best_clf_name = best_classifier_gridsearch.classifier\n",
    "    \n",
    "    # Apply the classification on the test subject\n",
    "    print(f\"P{participant} - Best clf: {best_clf_name}\\n\\tBest performance {best_classifier_gridsearch.mean_test_accuracy.values}\\n\\tBest params {best_classifier_gridsearch.params.values}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots and tables\n",
    "\n",
    "1. What is the mean/std accuracy/f1-score across the 39 participants?\n",
    "2. What is the mean f1-score of each classifier (best at hyperparam optimization process) per data modality among the 39 participants?\n",
    "3. Best combination of classifier/data modality per participant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the results based on the hyperparameters combination with highest f1-score\n",
    "df_summary_classif = df_results_hyperparam_opt[ (df_results_hyperparam_opt.rank_test_f1_macro == 1) ]\n",
    "df_summary_classif.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of test results\n",
    "data_results_main = df_summary_classif.groupby([\"test_subject_id\", \"data_modality\",\"classifier\"]).first()[ [\"mean_test_accuracy\",\"mean_test_f1_macro\"] ]\n",
    "data_results_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "data_results_main = data_results_main.reset_index()\n",
    "data_results_main.columns = [\"Subject\", \"Data Modality\", \"Classifier\", \"Accuracy\", \"F1-score\"]\n",
    "data_results_main[\"Data Modality\"].replace( dict(zip([\"all\",\"emg_amp\",\"emg_cont\",\"hrv\",\"imu\"],[\"All\",\"EMG A\", \"EMG C\", \"HRV\", \"IMU\"])), inplace=True )\n",
    "data_results_main = data_results_main.set_index([\"Subject\", \"Data Modality\", \"Classifier\"])\n",
    "data_results_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table with scores per data modality and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = data_results_main.stack().reset_index()\n",
    "df_table = df_table.rename(columns={\"level_3\":\"Metric\",0:\"Value\"})\n",
    "df_table_mean = df_table.groupby([\"Data Modality\",\"Classifier\",\"Metric\"]).mean().drop(\"Subject\",axis=1).unstack([\"Classifier\",\"Metric\"])\n",
    "df_table_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table_std = df_table.groupby([\"Data Modality\",\"Classifier\",\"Metric\"]).std().drop(\"Subject\",axis=1).unstack([\"Classifier\",\"Metric\"])\n",
    "df_table_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a text\n",
    "df_str_mean = df_table_mean.apply(lambda x: ['%.2f'%v for v in x.values])\n",
    "df_str_std = df_table_std.apply(lambda x: ['%.2f'%v for v in x.values])\n",
    "\n",
    "# df_results = (df_str_mean + \"(\" + df_str_std + \")\")\n",
    "df_results = (df_str_mean)\n",
    "\n",
    "# Rename columns\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LaTeX table\n",
    "FEATURE_BASED_CLASSIFIERS_RESULTS_FILENAME = gen_path_results(\"results-classificaiton-table\", extension=\".tex\")\n",
    "df_results.style.to_latex(FEATURE_BASED_CLASSIFIERS_RESULTS_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean f1-score among participants\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores across participants\n",
    "df_temp_mean = data_results_main[\"F1-score\"].reset_index()\n",
    "df_temp_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1, 1, figsize=(9,4))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.barplot(ax = axes, data = df_temp_mean, x=\"Data Modality\", y=\"F1-score\", hue=\"Classifier\",\n",
    "                errorbar=\"sd\", errwidth=1, capsize=0.1, palette=\"Set1\")\n",
    "# plt.legend(bbox_to_anchor=(0, 1, 1, 0), loc=\"lower left\", mode=\"expand\", ncol=4)\n",
    "plt.xlabel(None)\n",
    "plt.grid(True)\n",
    "plt.ylim([0,1])\n",
    "\n",
    "plt.tight_layout()\n",
    "save_path_plot = gen_path_plot(f\"results-classif-barplot-per-data-modality\")\n",
    "plt.savefig(save_path_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best combination data modality/classifier per participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-score per subject\n",
    "fig,axes = plt.subplots(1, 1, figsize=(11,6))\n",
    "df_heatmap = df_temp_mean.pivot(index=[\"Data Modality\",\"Classifier\"], columns=\"Subject\", values=\"F1-score\")\n",
    "sns.heatmap(df_heatmap, ax=axes, cmap=\"Spectral\", vmin=0, vmax=1)\n",
    "plt.xlabel(None)\n",
    "plt.ylabel(None)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_path_plot = gen_path_plot(f\"results-classif-heatmap-per-subject\")\n",
    "plt.savefig(save_path_plot, bbox_inches='tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">> FINISHED WITHOUT ERRORS!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "f3aec1f4fef7a88c2258d5b84a8b82909f076cff2bcb16988c856ebc42b66954"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "e8be33a246b23b79b36555b26872bcac753cc5311773880d7b4abb5b9e455248"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
